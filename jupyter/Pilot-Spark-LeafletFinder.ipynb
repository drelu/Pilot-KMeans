{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDAnalysis and Pilot-In-Memory\n",
    "\n",
    "\n",
    "The main performance bottleneck of the current MDAnalysis implementation is the construction of the graph using NetworkX taking ~78% of the overall runtime.\n",
    "\n",
    "\n",
    "**Beckstein Profiling:**\n",
    "\n",
    "    47        10           33      3.3      0.0      if adj is None:\n",
    "    48        10        66544   6654.4      0.0          x = atoms.positions\n",
    "\n",
    "    54        10     58689221 5868922.1     18.8          adj = (MDAnalysis.core.parallel.distances.distance_array(x, x, box=box) < cutoff)\n",
    "    \n",
    "    58        10           78      7.8      0.0      adjk = adj if Nmax is None else adj[:Nmax, :Nmax] \n",
    "    59        10    243009076 24300907.6   77.9      graph = nx.Graph(adjk)\n",
    "    60        10      4346636 434663.6      1.4      subgraphs = nx.connected_components(graph)\n",
    "    61        49        83597   1706.1      0.0      indices = [np.sort(g) for g in subgraphs]\n",
    "    62        49      5694698 116218.3      1.8      return [atoms[group].residues for group in indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LeafletFinder NetworkX Implementation Profiling\n",
    "\n",
    "see https://code.google.com/p/mdanalysis/\n",
    "\n",
    "Profile default implementation based on [NetworkX](https://networkx.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILENAME=\"../data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "!head -n 5 {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%time\n",
    "import networkx as NX\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "start = time.time()\n",
    "nxg = NX.read_edgelist(FILENAME, delimiter=\",\")\n",
    "end_read = time.time()\n",
    "NX.draw(nxg, pos=NX.spring_layout(nxg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "degree_sequence=sorted(NX.degree(nxg).values(),reverse=True) # degree sequence\n",
    "print \"Degree sequence\", degree_sequence\n",
    "print \"Length: %d\" % len(degree_sequence)\n",
    "\n",
    "dmax=max(degree_sequence)\n",
    "\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Degree Histogram\")\n",
    "plt.ylabel(\"Degree\")\n",
    "plt.xlabel(\"Node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "components = NX.connected_components(nxg)\n",
    "end_created = time.time()\n",
    "count = 0\n",
    "for component in components:\n",
    "    print str(sorted(component))\n",
    "    count = count + 1\n",
    "end_connected = time.time()\n",
    "print (\"Number of Nodes: \" + str(NX.number_of_nodes(nxg)))\n",
    "print (\"Number of Edges: \" + str(NX.number_of_edges(nxg)))\n",
    "print (\"Connected Components: \" + str(count))\n",
    "print (\"Runtime: \" + str((end_connected-start)))\n",
    "print (\"Graph Creation Runtime: \" + str((end_created-start)))\n",
    "print (\"Connected Components Runtime: \" + str((end_connected - end_created)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pilot_hadoop import PilotComputeService\n",
    "from IPython.display import HTML\n",
    "\n",
    "os.environ[\"SAGA_VERBOSE\"]=\"100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pilot-Spark and Pilot-InMemory Implementation\n",
    "\n",
    "Setup Spark cluster on local machine or HPC resource. Execute **either** 2.1.1 or 2.1.2\n",
    "\n",
    "### 2.1.1 Start Spark Cluster using Pilot-Spark (Stampede)\n",
    "\n",
    "see https://github.com/drelu/saga-hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pilot_compute_description = {\n",
    "                            \"resource_url\":\"fork://localhost\",\n",
    "                            \"number_cores\": 1,\n",
    "                            \"cores_per_node\":1,\n",
    "                            \"type\":\"spark\"\n",
    "                            }\n",
    "pilot = PilotComputeService.create_pilot(pilot_compute_description);\n",
    "\n",
    "# print out details of Pilot-Spark\n",
    "details = pilot.get_details()\n",
    "HTML(\"<a target='blank' href='%s'>Spark Web UI</a>\"%details[\"web_ui_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Start Spark Cluster inside YARN (Chameleon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK Home: /usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6\n",
      "SPARK HOME: /usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6\n",
      "PYTHONPATH: /usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python:/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip\n",
      "Spark Startup, 10.44\n"
     ]
    }
   ],
   "source": [
    "%run util/init_spark.py\n",
    "\n",
    "NUMBER_EXECUTORS=4\n",
    "\n",
    "from pilot_hadoop import PilotComputeService as PilotSparkComputeService\n",
    "\n",
    "pilotcompute_description = {\n",
    "    \"service_url\": \"yarn-client://yarn.radical-cybertools.org\",\n",
    "    \"number_of_processes\": NUMBER_EXECUTORS,\n",
    "    \"physical_memory_per_process\": \"3G\" \n",
    "}\n",
    "\n",
    "print \"SPARK HOME: %s\"%os.environ[\"SPARK_HOME\"]\n",
    "print \"PYTHONPATH: %s\"%os.environ[\"PYTHONPATH\"]\n",
    "\n",
    "start = time.time()\n",
    "pilot_spark = PilotSparkComputeService.create_pilot(pilotcompute_description=pilotcompute_description)\n",
    "sc = pilot_spark.get_spark_context()\n",
    "print \"Spark Startup, %.2f\"%(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd=sc.parallelize(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python 2.7.5\\n', 'Python 2.7.5\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "rdd.map(lambda a: subprocess.check_output('python --version', shell=True, stderr=subprocess.STDOUT)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Distance Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "coord = np.loadtxt(\"vesicle_1_5M_373_stride1000.xtc_145746Atoms.np_txt\", dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coord_broadcast = sc.broadcast(coord[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coord_all = coord_broadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 453.69998169,  525.39996338,   53.5       ],\n",
       "       [ 448.5       ,  524.39996338,   49.5       ],\n",
       "       [ 434.29998779,  508.99996948,   56.59999847],\n",
       "       [ 459.29998779,  516.79998779,   55.79999924]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_all[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_rdd=sc.parallelize(range(len(coord_broadcast.value)), NUMBER_EXECUTORS)\n",
    "part_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code wraps the MDAnalysis functions into Spark code that is executed in a data-parallel way either on an individual or a batch of points (1-D partitioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python2.7/site-packages/MDAnalysis/core/distances.py:9: DeprecationWarning: distances has moved to MDAnalysis.lib.distances and will be removed from here in release 1.0\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from MDAnalysis.core.distances import distance_array, self_distance_array\n",
    "from MDAnalysis.analysis.distances import contact_matrix\n",
    "import scipy.sparse\n",
    "from scipy.spatial.distance import cdist\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "\n",
    "cutoff = 15.0\n",
    "\n",
    "################################################################################\n",
    "# Process batch of points (a partition of the RDD)\n",
    "\n",
    "def get_edges_partition(adjacency_matrix, cutoff=15.0):\n",
    "    it = np.nditer(adjacency_matrix, flags=['multi_index'])\n",
    "    edge_list = []\n",
    "    while not it.finished:\n",
    "        value = it[0]\n",
    "        if cutoff < value:\n",
    "            # only connect 1 undirectional edge, e.g. <0,1>, but not <1,0>'\n",
    "            if it.multi_index[0]<=it.multi_index[1]:\n",
    "                edge_list.append((it.multi_index[0], it.multi_index[1]))\n",
    "                #print \"%d <%s>\" % (it[0], it.multi_index),\n",
    "        it.iternext()\n",
    "    return edge_list\n",
    "\n",
    "def compute_distance_partition(iterator):\n",
    "    \"\"\"Partition points in 1-D\"\"\"\n",
    "    min_value=sys.maxint\n",
    "    max_value=-sys.maxint-1\n",
    "    for i in iterator:\n",
    "        if i < min_value:\n",
    "            min_value = i\n",
    "        if i > max_value:\n",
    "            max_value = i\n",
    "    \n",
    "    # 2-D Partitioning\n",
    "    coord_all = coord_broadcast.value\n",
    "    coord_part = coord_all[min_value:max_value]\n",
    "    #print \"**All**\"\n",
    "    #print str(coord_all)\n",
    "    #print \"**Part**\"\n",
    "    #print str(coord_part)\n",
    "    #adj=contact_matrix(coord_part, returntype=\"sparse\")\n",
    "    #adj = distance_array(coord_part, coord_all, box=None)\n",
    "    adj = cdist(coord_part, coord_all)\n",
    "    #print \"**scipy.spatial.distance.cdist**\"\n",
    "    #print(adj)\n",
    "    #adj2 = distance_array(coord_part, coord_all, box=None)\n",
    "    #print \"**MDAnalysis**\"\n",
    "    #print(adj2)\n",
    "    \n",
    "    edge_list = get_edges_partition(adj)\n",
    "    del coord_part\n",
    "    del coord_all\n",
    "    del adj\n",
    "    gc.collect()\n",
    "    return edge_list\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Process one point at a time\n",
    "def get_edges_point(point_index, adjacency_matrix, cutoff=15.0):\n",
    "    edge_list = []\n",
    "    for index, i in np.ndenumerate(adjacency_matrix):\n",
    "        #print (\"Index: %d, Value: %d\"%(index[i], i))\n",
    "        #if point_index<=index[1] and i<cutoff:\n",
    "        if i==True and point_index<=index[1]:\n",
    "            # Attention we only compute the upper half of the adjacency matrix\n",
    "            # thus we need to offset the target edge vertice by point_index\n",
    "            edge_list.append((point_index, point_index+index[1]))\n",
    "    #del adjacency_matrix\n",
    "    return edge_list\n",
    "\n",
    "def compute_distance(point_index):\n",
    "    # 1-D Partitioning\n",
    "    coord_all = coord_broadcast.value\n",
    "    coord_part = coord_all[point_index-1:point_index]\n",
    "    #adj = (distance_array(coord_part, coord_all[point_index:], box=None) < cutoff)\n",
    "    adj = (cdist(coord_part, coord_all) < cutoff)\n",
    "    #adj = cdist(coord_part, coord_all)\n",
    "    edge_list = get_edges_point(point_index, adj)\n",
    "    del adj\n",
    "    #del coord_part\n",
    "    #del coord_all\n",
    "    #gc.collect()\n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115093\n",
      "ComputeDistanceSpark, 20000, 96, 378.80\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "edges_list=part_rdd.map(compute_distance).flatMap(lambda a: a).collect()\n",
    "#edges_list_spark=part_rdd.mapPartitions(compute_distance_partition).collect()\n",
    "print str(len(edges_list))\n",
    "print \"ComputeDistanceSpark, %d, %d, %.2f\"%(len(coord_all), NUMBER_EXECUTORS, (time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115093\n",
      "ComputeDistanceSpark, 20000, 96, 114.90\n"
     ]
    }
   ],
   "source": [
    "#start = time.time()\n",
    "#edges_list_local = compute_distance_partition(iter(range(20000)))\n",
    "#print str(len(edges_list))\n",
    "#print \"ComputeDistanceLocal, %d, %.2f\"%(len(coord_all),(time.time()-start))\n",
    "\n",
    "for i in range(1):\n",
    "    start = time.time()\n",
    "    edges_list=part_rdd.map(compute_distance).flatMap(lambda a: a).collect()\n",
    "    #edges_list_spark=part_rdd.mapPartitions(compute_distance_partition).collect()\n",
    "    print str(len(edges_list))\n",
    "    print \"ComputeDistanceSpark, %d, %d, %.2f\"%(len(coord_all), NUMBER_EXECUTORS, (time.time()-start))\n",
    "    del edges_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unoptimized using cartesian product\n",
    "\n",
    "Not good for sparse result data. Only usable on a very small sample:\n",
    "\n",
    "    sample=row_rdd.sample(False, 0.01, 81)\n",
    "    sample.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3] at mapPartitions at PythonMLLibAPI.scala:1480"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.mllib.linalg.distributed\n",
    "coord_matrix=pyspark.mllib.linalg.distributed.RowMatrix(sc.parallelize(coord[:200], 4))\n",
    "row_rdd=coord_matrix.rows\n",
    "row_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coord = np.loadtxt(\"md_centered.xtc_95Atoms.np_txt\", dtype='float32')\n",
    "#coord_str = np.array2string(coord, separator=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 36.83100128,  20.13500023,  37.15499878],\n",
       "       [ 46.68199921,  66.16099548,  63.26799774],\n",
       "       [ 67.522995  ,  68.6230011 ,  54.61399841],\n",
       "       [ 40.35400009,  14.70999908,  42.97800064],\n",
       "       [ 42.58300018,  36.86699677,  11.01300049],\n",
       "       [ 22.73099899,  54.88199615,  39.13800049],\n",
       "       [ 69.51100159,  34.54999924,  27.00699997],\n",
       "       [ 49.21099854,  27.49599838,  60.3809967 ],\n",
       "       [ 49.60199738,  20.21199989,   6.79399967],\n",
       "       [ 30.53499985,   4.55800009,   7.03599977],\n",
       "       [  9.27799988,   8.9829998 ,  38.02500153],\n",
       "       [  8.0369997 ,  66.09799957,  39.86299896],\n",
       "       [ 55.26000214,   4.08699989,  67.3999939 ],\n",
       "       [ 30.85899734,  59.57500076,  10.19099998],\n",
       "       [ 26.04100037,  13.67999935,  44.57299805],\n",
       "       [ 51.84799576,  22.13299942,  33.64899826],\n",
       "       [  7.06699944,  25.31699944,  59.99300003],\n",
       "       [ 23.29599762,  33.74100113,  74.61499786],\n",
       "       [ 64.36899567,  68.68099976,  30.96899986],\n",
       "       [ 18.94499969,  39.68700027,  11.47200012],\n",
       "       [ 37.95499802,  66.77300262,   7.17999983],\n",
       "       [ 22.54799843,  74.47099304,  60.92899704],\n",
       "       [ 55.18299866,  21.93300056,  61.58599854],\n",
       "       [ 75.72899628,  43.81999969,  14.63899994],\n",
       "       [  9.94299984,  35.36599731,  10.93400002],\n",
       "       [ 70.68699646,  47.62000275,  36.29100037],\n",
       "       [ 24.9659996 ,  17.31699944,  32.28699875],\n",
       "       [ 10.65999985,  19.49099922,  77.15799713],\n",
       "       [  7.39299965,  77.45999908,  67.5759964 ],\n",
       "       [ 10.67399979,  34.8030014 ,  42.38100052],\n",
       "       [ 64.94499969,  48.16400146,  73.0249939 ],\n",
       "       [ 10.95099926,  38.40499878,  67.23000336],\n",
       "       [ 45.51599884,  10.93599892,  75.00499725],\n",
       "       [ 31.23999786,  29.88599968,  70.15699768],\n",
       "       [ 13.13099957,  51.15999603,  36.03299713],\n",
       "       [  4.90299988,  32.88199997,  56.14099884],\n",
       "       [ 38.88800049,  40.06299973,  73.35399628],\n",
       "       [ 37.34199905,  29.98199844,  15.1079998 ],\n",
       "       [ 65.28499603,  28.09700012,  41.76499939],\n",
       "       [ 12.60700035,  11.28899956,   8.41899967],\n",
       "       [ 27.8579998 ,  76.57299805,  52.42099762],\n",
       "       [ 74.29699707,  42.03700256,  26.74300003],\n",
       "       [ 53.2519989 ,  15.68799973,  17.72800064],\n",
       "       [ 59.25499725,  25.63599777,  47.95199966],\n",
       "       [  4.15700006,  73.54100037,  73.79999542],\n",
       "       [ 74.47799683,  61.44599915,  49.42499542],\n",
       "       [ 46.22100067,  78.05999756,  66.58000183],\n",
       "       [  5.25699997,  55.84600067,   9.04199982],\n",
       "       [ 23.70300102,  54.54700089,   5.16900015],\n",
       "       [ 30.85199738,  73.47399902,  40.26599884],\n",
       "       [ 26.71099854,  21.6269989 ,  73.18199921],\n",
       "       [ 18.7689991 ,  43.25900269,  70.08699799],\n",
       "       [ 26.38000107,  59.40099716,  27.91999817],\n",
       "       [ 77.8279953 ,  20.2159996 ,   5.31700039],\n",
       "       [ 61.13499451,   4.54099989,  73.2029953 ],\n",
       "       [ 30.10099792,  12.98299885,  69.09400177],\n",
       "       [ 30.36399841,  53.48799896,  39.31800079],\n",
       "       [ 46.50299835,  60.35400009,   7.69700003],\n",
       "       [ 30.99799919,  58.72600174,  16.90499878],\n",
       "       [ 33.36700058,  65.58999634,  73.27399445],\n",
       "       [ 16.81899834,  27.96699905,  20.51699829],\n",
       "       [ 49.47799683,  13.76199913,  22.78100014],\n",
       "       [ 72.87999725,  61.85100174,   7.5539999 ],\n",
       "       [ 62.1590004 ,  35.95299911,  63.4489975 ],\n",
       "       [ 16.56999969,  59.59600067,  42.90199661],\n",
       "       [ 60.04699707,  56.05099487,  63.19599533],\n",
       "       [ 64.08699799,   4.19299984,  81.56600189],\n",
       "       [ 22.48399925,  25.30900002,  15.03199959],\n",
       "       [ 72.81299591,   3.36999989,  76.8279953 ],\n",
       "       [ 39.22699738,  74.10800171,   7.2329998 ],\n",
       "       [ 36.18599701,  30.15999985,   5.76499987],\n",
       "       [  5.16200018,  38.16500092,  24.05599785],\n",
       "       [ 74.56299591,   2.53499985,  41.33999634],\n",
       "       [ 55.29800034,  56.5909996 ,  25.76399994],\n",
       "       [ 23.72200012,  44.66199875,  79.147995  ],\n",
       "       [ 49.7179985 ,  75.67199707,  22.70400047],\n",
       "       [ 62.37799835,  48.75699997,   0.41299999],\n",
       "       [ 21.47899818,  17.53299904,   3.84199977],\n",
       "       [ 72.0759964 ,  64.21700287,  29.04899788],\n",
       "       [ 74.83499908,  20.36700058,  26.60400009],\n",
       "       [ 15.20800018,  47.80599976,  78.29699707],\n",
       "       [ 76.1499939 ,  23.95899963,  50.81999969],\n",
       "       [ 71.65699768,  18.61699867,  79.61499786],\n",
       "       [ 51.80500031,  64.40299988,  47.17199707],\n",
       "       [ 14.77899933,  50.98300171,  42.91999817],\n",
       "       [ 31.6629982 ,  76.67399597,  27.32799911],\n",
       "       [ 26.96899796,  22.11799812,  39.30400085],\n",
       "       [  9.93799973,  57.23300171,  63.06999588],\n",
       "       [ 14.72899914,  55.68799973,  17.63999939],\n",
       "       [ 18.72599983,   1.39599991,  53.63800049],\n",
       "       [ 24.00300026,  61.55500031,  49.13999939],\n",
       "       [ 27.8390007 ,  26.87199783,  45.96699524],\n",
       "       [ 61.81099701,  18.34899902,  41.61100006],\n",
       "       [ 75.16400146,  60.31900024,  22.36399841],\n",
       "       [ 77.53699493,  68.48199463,  68.04999542]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coord_str=[]\n",
    "for i in range(len(coord)):\n",
    "    coord_str.append(str(coord[i][0]) +\",\"+ str(coord[i][1]) +\",\"+ str(coord[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 36.83100128,  20.13500023,  37.15499878],\n",
       "       [ 46.68199921,  66.16100311,  63.26800156],\n",
       "       [ 67.52300262,  68.6230011 ,  54.61399841],\n",
       "       [ 40.35400009,  14.71000004,  42.97800064],\n",
       "       [ 42.58300018,  36.86700058,  11.01299953],\n",
       "       [ 22.7310009 ,  54.88199997,  39.13800049],\n",
       "       [ 69.51100159,  34.54999924,  27.00699997],\n",
       "       [ 49.21099854,  27.49600029,  60.38100052],\n",
       "       [ 49.60200119,  20.21199989,   6.79400015],\n",
       "       [ 30.53499985,   4.55800009,   7.03599977],\n",
       "       [  9.27799988,   8.9829998 ,  38.02500153],\n",
       "       [  8.0369997 ,  66.09799957,  39.86299896],\n",
       "       [ 55.25999832,   4.08699989,  67.40000153],\n",
       "       [ 30.85899925,  59.57500076,  10.19099998],\n",
       "       [ 26.04100037,  13.68000031,  44.57300186],\n",
       "       [ 51.84799957,  22.13299942,  33.64899826],\n",
       "       [  7.06699991,  25.31699944,  59.99300003],\n",
       "       [ 23.29599953,  33.74100113,  74.61499786],\n",
       "       [ 64.3690033 ,  68.68099976,  30.96899986],\n",
       "       [ 18.94499969,  39.68700027,  11.47200012],\n",
       "       [ 37.95500183,  66.77300262,   7.17999983],\n",
       "       [ 22.54800034,  74.47100067,  60.92900085],\n",
       "       [ 55.18299866,  21.93300056,  61.58599854],\n",
       "       [ 75.72899628,  43.81999969,  14.63899994],\n",
       "       [  9.94299984,  35.36600113,  10.93400002],\n",
       "       [ 70.68699646,  47.61999893,  36.29100037],\n",
       "       [ 24.9659996 ,  17.31699944,  32.28699875],\n",
       "       [ 10.65999985,  19.49099922,  77.15799713],\n",
       "       [  7.39300013,  77.45999908,  67.5759964 ],\n",
       "       [ 10.67399979,  34.8030014 ,  42.38100052],\n",
       "       [ 64.94499969,  48.16400146,  73.02500153],\n",
       "       [ 10.95100021,  38.40499878,  67.23000336],\n",
       "       [ 45.51599884,  10.93599987,  75.00499725],\n",
       "       [ 31.23999977,  29.88599968,  70.15699768],\n",
       "       [ 13.13099957,  51.15999985,  36.03300095],\n",
       "       [  4.90299988,  32.88199997,  56.14099884],\n",
       "       [ 38.88800049,  40.06299973,  73.35399628],\n",
       "       [ 37.34199905,  29.98200035,  15.1079998 ],\n",
       "       [ 65.28500366,  28.09700012,  41.76499939],\n",
       "       [ 12.60700035,  11.28899956,   8.41899967],\n",
       "       [ 27.8579998 ,  76.57299805,  52.42100143],\n",
       "       [ 74.29699707,  42.03699875,  26.74300003],\n",
       "       [ 53.2519989 ,  15.68799973,  17.72800064],\n",
       "       [ 59.25500107,  25.63599968,  47.95199966],\n",
       "       [  4.15700006,  73.54100037,  73.80000305],\n",
       "       [ 74.47799683,  61.44599915,  49.42499924],\n",
       "       [ 46.22100067,  78.05999756,  66.58000183],\n",
       "       [  5.25699997,  55.84600067,   9.04199982],\n",
       "       [ 23.70299911,  54.54700089,   5.16900015],\n",
       "       [ 30.85199928,  73.47399902,  40.26599884],\n",
       "       [ 26.71100044,  21.62700081,  73.18199921],\n",
       "       [ 18.7689991 ,  43.25899887,  70.08699799],\n",
       "       [ 26.37999916,  59.40100098,  27.92000008],\n",
       "       [ 77.82800293,  20.2159996 ,   5.31699991],\n",
       "       [ 61.13499832,   4.54099989,  73.20300293],\n",
       "       [ 30.10099983,  12.9829998 ,  69.09400177],\n",
       "       [ 30.36400032,  53.48799896,  39.31800079],\n",
       "       [ 46.50299835,  60.35400009,   7.69700003],\n",
       "       [ 30.99799919,  58.72600174,  16.90500069],\n",
       "       [ 33.36700058,  65.58999634,  73.27400208],\n",
       "       [ 16.81900024,  27.96699905,  20.5170002 ],\n",
       "       [ 49.47800064,  13.76200008,  22.78100014],\n",
       "       [ 72.87999725,  61.85100174,   7.5539999 ],\n",
       "       [ 62.1590004 ,  35.95299911,  63.44900131],\n",
       "       [ 16.56999969,  59.59600067,  42.90200043],\n",
       "       [ 60.04700089,  56.05099869,  63.19599915],\n",
       "       [ 64.08699799,   4.19299984,  81.56600189],\n",
       "       [ 22.48399925,  25.30900002,  15.03199959],\n",
       "       [ 72.81300354,   3.36999989,  76.82800293],\n",
       "       [ 39.22700119,  74.10800171,   7.2329998 ],\n",
       "       [ 36.18600082,  30.15999985,   5.76499987],\n",
       "       [  5.16200018,  38.16500092,  24.05599976],\n",
       "       [ 74.56300354,   2.53500009,  41.34000015],\n",
       "       [ 55.29800034,  56.5909996 ,  25.76399994],\n",
       "       [ 23.72200012,  44.66199875,  79.14800262],\n",
       "       [ 49.7179985 ,  75.67199707,  22.70400047],\n",
       "       [ 62.37799835,  48.75699997,   0.41299999],\n",
       "       [ 21.47900009,  17.53300095,   3.84200001],\n",
       "       [ 72.0759964 ,  64.21700287,  29.04899979],\n",
       "       [ 74.83499908,  20.36700058,  26.60400009],\n",
       "       [ 15.20800018,  47.80599976,  78.29699707],\n",
       "       [ 76.15000153,  23.95899963,  50.81999969],\n",
       "       [ 71.65699768,  18.61700058,  79.61499786],\n",
       "       [ 51.80500031,  64.40299988,  47.17200089],\n",
       "       [ 14.77900028,  50.98300171,  42.91999817],\n",
       "       [ 31.66300011,  76.6740036 ,  27.32799911],\n",
       "       [ 26.96899986,  22.11800003,  39.30400085],\n",
       "       [  9.93799973,  57.23300171,  63.06999969],\n",
       "       [ 14.72900009,  55.68799973,  17.63999939],\n",
       "       [ 18.72599983,   1.39600003,  53.63800049],\n",
       "       [ 24.00300026,  61.55500031,  49.13999939],\n",
       "       [ 27.8390007 ,  26.87199974,  45.96699905],\n",
       "       [ 61.81100082,  18.34900093,  41.61100006],\n",
       "       [ 75.16400146,  60.31900024,  22.36400032],\n",
       "       [ 77.53700256,  68.48200226,  68.05000305]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count, 26.45\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sc.parallelize(range(200), 4).count()\n",
    "print \"Count, %.2f\"%((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:703)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:702)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:702)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1514)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1438)\n\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1724)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1185)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1723)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:146)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1921)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:909)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:908)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0d01d3215655>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrow_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcartesian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_rdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \"\"\"\n\u001b[0;32m    772\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:703)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:702)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:702)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1514)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1438)\n\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1724)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1185)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1723)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:146)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1921)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:909)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:908)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "row_rdd.cartesian(row_rdd).map(lambda a: a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "distances=  row_rdd.cartesian(row_rdd).\\\n",
    "            map(lambda a: (a[0].squared_distance(a[1]))).\\\n",
    "            filter(lambda a: a>15.0).\\\n",
    "            saveAsTextFile(\"distances.csv\")\n",
    "print \"ComputeDistance, %.2f\"%(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-D Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_partitions=4\n",
    "\n",
    "def compute_distance_2d(partition_index):\n",
    "    # 2-D Partitioning\n",
    "    coord_all = coord_broadcast.value[:100]\n",
    "    length = len(coord_all)\n",
    "    # identify square to work on    \n",
    "    xdim = math.sqrt(num_partitions)\n",
    "    ydim = math.sqrt(num_partitions)\n",
    "    xdim/partition_index\n",
    "    len=len(coord_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Leaflet Finder Pilot-InMemory Implementation (Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from distributed_inmem.dataunit_spark import DistributedInMemoryDataUnit\n",
    "import time\n",
    "\n",
    "FILENAME=\"../data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "\n",
    "FILENAME_ALL_EDGES=\"../data/mdanalysis/small/graph_edges_95_215_alledges.csv\"\n",
    "du = DistributedInMemoryDataUnit(name=\"LeafletFinderGraph\", sc=sc)\n",
    "\n",
    "#DistributedInMemoryDataUnit.spark_context.version\n",
    "\n",
    "f = open(FILENAME_ALL_EDGES)\n",
    "graph = f.readlines()\n",
    "du.load(graph)\n",
    "f.close()\n",
    "\n",
    "def identityMapper(edge, args):\n",
    "    #print edge\n",
    "    #comp = edge.strip().split(\",\")\n",
    "    #return (int(comp[0]), int(comp[1]))\n",
    "    return eval(str(edge))\n",
    "\n",
    "def groupByVertex(data):\n",
    "    print(\"Call reduce on: \" + str(data))\n",
    "    \n",
    "\n",
    "new_iteration_needed = du.sc.accumulator(0)\n",
    "\n",
    "# check for smaller keys in each set\n",
    "def process_vertex(vertex):\n",
    "    \"\"\" pass single vertex and its adjecent vertices\n",
    "        e.g.: (0, [0, 67, 14])\n",
    "    \"\"\"\n",
    "    global new_iteration_needed\n",
    "    vertex = eval(vertex)\n",
    "    source = int(vertex[0])\n",
    "    dest= sorted([int(i) for i in vertex[1]])\n",
    "    local_max = False\n",
    "    \n",
    "    first_edge_destination = int(dest[0])\n",
    "    new_vertices = []    \n",
    "    print \"*********Source: %d First Edge Dest: %d\"%(source, first_edge_destination) \n",
    "    if source <= first_edge_destination:\n",
    "        local_max = True\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "            \n",
    "\n",
    "    print \"Process: \" + str(vertex) + \" Local Max: \" + str(local_max)\n",
    "    last_edge_destination = first_edge_destination\n",
    "\n",
    "    for current_destination in vertex[1]:\n",
    "        print \"Current destination: %s\"%str(current_destination)\n",
    "        current_destination = int(current_destination)\n",
    "        if current_destination == last_edge_destination: \n",
    "            continue\n",
    "        \n",
    "        if local_max == True:\n",
    "            edge = (source, current_destination)\n",
    "            new_vertices.append(edge)\n",
    "        else:\n",
    "            new_vertices.append((first_edge_destination, current_destination))\n",
    "            new_vertices.append((current_destination, first_edge_destination))\n",
    "            print \"Add 1 to accumulator\"\n",
    "            new_iteration_needed.add(1)\n",
    "\n",
    "        last_edge_destination = current_destination\n",
    "    \n",
    "    if ((not local_max) and (source < last_edge_destination)):\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "    \n",
    "    print \"Return new vertices: \" + str(new_vertices)\n",
    "    return new_vertices\n",
    "\n",
    "\n",
    "#process_vertex(\"('19', ['19', '7', '9', '41'])\")\n",
    "num_iterations=0\n",
    "start = time.time()\n",
    "while True:\n",
    "    old_accum_value = new_iteration_needed.value\n",
    "    print \"*********** Start iteration: %d \" % num_iterations\n",
    "    future_result = du.map_pilot(identityMapper, None, number_of_compute_units=2)\n",
    "    result_du=future_result.result()[0]\n",
    "    future_result = result_du.reduce_pilot(process_vertex, number_of_compute_units=2)\n",
    "    output = future_result.result()\n",
    "    output.export()   \n",
    "    du = output\n",
    "    num_iterations = num_iterations + 1\n",
    "    print \"New iteration accum: %d old value: %d\"%(new_iteration_needed.value, old_accum_value)\n",
    "    if old_accum_value < new_iteration_needed.value:\n",
    "        #print \"Accumulator value was increased. New iteration.\"\n",
    "        continue        \n",
    "        #pass\n",
    "    else:\n",
    "        break\n",
    "    break\n",
    "end = time.time()\n",
    "print \"Final results: \"\n",
    "num_components=du.data.groupByKey().count()\n",
    "print \"Finished after %d Iterations. Found %d components. Time: %.2f\"%(num_iterations, num_components, (end-start)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "du.data.groupByKey().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Native Spark Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 luckow hdfs   29855041 2015-11-25 00:03 /data/mdanalysis/large/graph_edges_145746_1012872.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /data/mdanalysis/large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/mdanalysis/large/graph_edges_145746_1012872.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Load data from text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [0, 14, 67]), (2, [2, 13, 34, 62]), (4, [4, 27, 33, 68, 94]), (6, [6, 57, 64, 67]), (8, [8, 46, 69, 88, 93]), (10, [10, 31, 40, 71]), (12, [7, 12, 30, 50, 91]), (14, [0, 14, 48, 64]), (16, [16, 40]), (18, [18]), (20, [20, 85]), (22, [22, 54]), (24, [24, 62]), (26, [26, 49, 57, 70]), (28, [11, 17, 28, 69]), (30, [9, 12, 23, 30, 50]), (32, [32, 53]), (34, [2, 34]), (36, [36, 47, 84, 92]), (38, [9, 23, 38]), (40, [10, 16, 31, 40, 71]), (42, [42, 73]), (44, [44, 68]), (46, [8, 46]), (48, [14, 48, 49]), (50, [12, 23, 30, 50]), (52, [29, 52, 79, 89]), (54, [22, 54, 87]), (56, [56, 76]), (58, [58, 81]), (60, [17, 60, 69, 81]), (62, [2, 24, 25, 62, 79, 89]), (64, [6, 14, 57, 64, 67]), (66, [51, 66]), (68, [4, 44, 68, 72, 94]), (70, [26, 70, 79, 86]), (72, [68, 72]), (74, [43, 47, 53, 74, 82]), (76, [56, 76]), (78, [78, 92]), (80, [43, 80]), (82, [74, 77, 82]), (84, [36, 63, 84]), (86, [70, 79, 86]), (88, [8, 69, 88, 93]), (90, [11, 55, 75, 90]), (92, [36, 47, 78, 92]), (94, [3, 4, 33, 59, 68, 94]), (1, [1, 41]), (3, [3, 39, 59, 94]), (5, [5, 29, 37, 61]), (7, [7, 12, 19]), (9, [9, 19, 23, 30, 38]), (11, [11, 28, 55, 90, 93]), (13, [2, 13, 25]), (15, [15]), (17, [17, 28, 60, 69]), (19, [7, 9, 19, 41]), (21, [21]), (23, [9, 23, 30, 38, 50, 51]), (25, [13, 25, 61, 62, 89]), (27, [4, 27, 33]), (29, [5, 29, 52, 61, 65]), (31, [10, 31, 40]), (33, [4, 27, 33, 59, 91, 94]), (35, [35, 45, 63]), (37, [5, 37]), (39, [3, 39, 87]), (41, [1, 19, 41]), (43, [43, 47, 74, 80]), (45, [35, 45]), (47, [36, 43, 47, 53, 74, 92]), (49, [26, 48, 49]), (51, [23, 51, 66]), (53, [32, 47, 53, 74]), (55, [11, 55, 90, 93]), (57, [6, 26, 57, 64]), (59, [3, 33, 59, 91, 94]), (61, [5, 25, 29, 61, 89]), (63, [35, 63, 84]), (65, [29, 65]), (67, [0, 6, 64, 67]), (69, [8, 17, 28, 60, 69, 88]), (71, [10, 40, 71]), (73, [42, 73, 81]), (75, [75, 85, 90]), (77, [77, 82]), (79, [52, 62, 70, 79, 86, 89]), (81, [58, 60, 73, 81]), (83, [83]), (85, [20, 75, 85]), (87, [39, 54, 87]), (89, [25, 52, 61, 62, 79, 89]), (91, [12, 33, 59, 91]), (93, [8, 11, 55, 88, 93])]\n",
      "CPU times: user 12 ms, sys: 743 µs, total: 12.7 ms\n",
      "Wall time: 680 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FILENAME=\"/data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "data = sc.textFile(FILENAME).map(lambda line: [int(i) for i in line.split(\",\")])\n",
    "# add backward edges\n",
    "data = data.flatMap(lambda v: [(v[0],v[1]),(v[1],v[0])])\n",
    "\n",
    "#data.saveAsTextFile(\"../data/mdanalysis/small/graph_edges_95_215_alledges.csv\")\n",
    "#data = data.filter(lambda v: v[0] != v[1])\n",
    "#print data.collect()\n",
    "\n",
    "data_grouped = data.groupByKey().mapValues(lambda a: sorted(set(a)))\n",
    "print data_grouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Connected Component Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Start iteration: 0 \n",
      "*********** Start iteration: 1 \n",
      "*********** Start iteration: 2 \n",
      "*********** Start iteration: 3 \n",
      "*********** Start iteration: 4 \n",
      "*********** Start iteration: 5 \n",
      "Finished after 6 Iterations. Found 10 components. Time: 2.39\n"
     ]
    }
   ],
   "source": [
    "new_iteration_needed = sc.accumulator(0)\n",
    "# check for smaller keys in each set\n",
    "def process_vertex(vertex):\n",
    "    \"\"\" pass single vertex and its adjecent vertices\n",
    "        e.g.: (0, [0, 67, 14])\n",
    "    \"\"\"\n",
    "    global new_iteration_needed\n",
    "    source = vertex[0]\n",
    "    local_max = False\n",
    "    \n",
    "    first_edge_destination = vertex[1][0]\n",
    "    new_vertices = []    \n",
    "    print \"*********Source: %d First Edge Dest: %d\"%(source, first_edge_destination) \n",
    "    if source <= first_edge_destination:\n",
    "        local_max = True\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "            \n",
    "    #pdb.set_trace()\n",
    "    print \"Process: \" + str(vertex) + \" Local Max: \" + str(local_max)\n",
    "    last_edge_destination = first_edge_destination\n",
    "\n",
    "    #if vertex[1]==None or len(vertex[1])<=1:\n",
    "    #    new_vertices.append((source, source))   \n",
    "    for current_destination in vertex[1]:\n",
    "        #print \"Current destination: %s\"%str(current_destination)\n",
    "        if current_destination == last_edge_destination: \n",
    "            continue\n",
    "        \n",
    "        if local_max == True:\n",
    "            edge = (source, current_destination)\n",
    "            new_vertices.append(edge)\n",
    "        else:\n",
    "            new_vertices.append((first_edge_destination, current_destination))\n",
    "            new_vertices.append((current_destination, first_edge_destination))\n",
    "            print \"Add 1 to accumulator\"\n",
    "            new_iteration_needed.add(1)\n",
    "\n",
    "        last_edge_destination = current_destination\n",
    "    \n",
    "    if ((not local_max) and (source < last_edge_destination)):\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "    \n",
    "    #print \"Return new vertices: \" + str(new_vertices)\n",
    "    return new_vertices\n",
    "\n",
    "\n",
    "#process_vertex((19, [7, 9, 19, 41]))\n",
    "num_iterations=0\n",
    "cc = data_grouped\n",
    "start = time.time()\n",
    "while True:\n",
    "    old_accum_value = new_iteration_needed.value\n",
    "    print \"*********** Start iteration: %d \" % num_iterations\n",
    "    #print \"Accum before iteration: \" + str(old_accum_value)\n",
    "    cc = cc.flatMap(lambda v: process_vertex(v))\\\n",
    "           .groupByKey()\\\n",
    "           .mapValues(lambda a: sorted(set(a)))\n",
    "    cc.collect()\n",
    "    num_iterations = num_iterations + 1\n",
    "    #print \"New iteration accum: %d old value: %d\"%(new_iteration_needed.value, old_accum_value)\n",
    "    if old_accum_value < new_iteration_needed.value:\n",
    "        #print \"Accumulator value was increased. New iteration.\"\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "end = time.time()\n",
    "\n",
    "print \"Finished after %d Iterations. Found %d components. Time: %.2f\"%(num_iterations, cc.count(), (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Scratch Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start = df_grouped\n",
    "result=start.flatMap(lambda v: (v[0], v[1])).map(lambda v: v<start_index).countByValue()\n",
    "\n",
    "print sttrresult\n",
    "\n",
    "local_max = not result.has_key(True)\n",
    "\n",
    "print \"Local Max: \" + str(local_max) + \" Smaller Index: \" + str(result.has_key(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "            StructField(\"source\", IntegerType(), True),\n",
    "            StructField(\"destination\", IntegerType(), True)\n",
    "        ])\n",
    "df = sqlCtx.createDataFrame(data, schema)\n",
    "df.explain()\n",
    "schema_grouped = StructType([\n",
    "            StructField(\"source\", IntegerType(), True),\n",
    "            StructField(\"destination\", ArrayType(IntegerType()), True)\n",
    "        ])\n",
    "df_grouped = sqlCtx.createDataFrame(data_grouped, schema_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "t = udf(lambda s: str(s), StringType())\n",
    "slen = udf(lambda s: Column(len(s)), IntegerType())\n",
    "\n",
    "#df.groupBy(\"source\").collect()\n",
    "#df.groupBy(\"source\").agg(df.source, t(df.source))\n",
    "\n",
    "c = df.groupBy(df.source).agg(col(\"source\"), slen(df.destination))\n",
    "\n",
    "#c = df.agg(col(\"source\"), t(df.destination).alias('counts'))\n",
    "c.head(5)\n",
    "\n",
    "\n",
    "#c = df.groupBy(df.source).lit(df.destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertices = df.select(df[\"source\"]).unionAll(df.select(df[\"destination\"]))\n",
    "vertices = di_source.distinct()\n",
    "\n",
    "print \"Number of vertices: %d\"%(vertices.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphlab import SGraph, SFrame\n",
    "from graphlab import connected_components\n",
    "\n",
    "PROBLEM={\"small\": \"./data/mdanalysis/small/graph_edges_95_215.csv\",\n",
    "         \"medium\":\"./data/mdanalysis/medium/graph_edges_24056_71826.csv\"}\n",
    "\n",
    "d =datetime.datetime.now()\n",
    "RESULTSFILE = \"results-\" + d.strftime(\"%Y%m%d-%H%M%S\") + \".csv\"\n",
    "REPEATS=5\n",
    "\n",
    "start = time.time()\n",
    "data = SFrame.read_csv(filename, header=False)\n",
    "sg = SGraph().add_edges(data, src_field=\"X1\", dst_field=\"X2\")\n",
    "end_read=time.time()\n",
    "cc = connected_components.create(sg)\n",
    "s=cc[\"component_size\"]\n",
    "end_connected = time.time()\n",
    "print cc\n",
    "print s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
