{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDAnalysis and Pilot-In-Memory\n",
    "\n",
    "\n",
    "The main performance bottleneck of the current MDAnalysis implementation is the construction of the graph using NetworkX taking ~78% of the overall runtime.\n",
    "\n",
    "\n",
    "**Beckstein Profiling:**\n",
    "\n",
    "    47        10           33      3.3      0.0      if adj is None:\n",
    "    48        10        66544   6654.4      0.0          x = atoms.positions\n",
    "\n",
    "    54        10     58689221 5868922.1     18.8          adj = (MDAnalysis.core.parallel.distances.distance_array(x, x, box=box) < cutoff)\n",
    "    \n",
    "    58        10           78      7.8      0.0      adjk = adj if Nmax is None else adj[:Nmax, :Nmax] \n",
    "    59        10    243009076 24300907.6   77.9      graph = nx.Graph(adjk)\n",
    "    60        10      4346636 434663.6      1.4      subgraphs = nx.connected_components(graph)\n",
    "    61        49        83597   1706.1      0.0      indices = [np.sort(g) for g in subgraphs]\n",
    "    62        49      5694698 116218.3      1.8      return [atoms[group].residues for group in indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LeafletFinder NetworkX Implementation Profiling\n",
    "\n",
    "see https://code.google.com/p/mdanalysis/\n",
    "\n",
    "Profile default implementation based on [NetworkX](https://networkx.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILENAME=\"../data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "!head -n 5 {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%time\n",
    "import networkx as NX\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "start = time.time()\n",
    "nxg = NX.read_edgelist(FILENAME, delimiter=\",\")\n",
    "end_read = time.time()\n",
    "NX.draw(nxg, pos=NX.spring_layout(nxg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "degree_sequence=sorted(NX.degree(nxg).values(),reverse=True) # degree sequence\n",
    "print \"Degree sequence\", degree_sequence\n",
    "print \"Length: %d\" % len(degree_sequence)\n",
    "\n",
    "dmax=max(degree_sequence)\n",
    "\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Degree Histogram\")\n",
    "plt.ylabel(\"Degree\")\n",
    "plt.xlabel(\"Node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "components = NX.connected_components(nxg)\n",
    "end_created = time.time()\n",
    "count = 0\n",
    "for component in components:\n",
    "    print str(sorted(component))\n",
    "    count = count + 1\n",
    "end_connected = time.time()\n",
    "print (\"Number of Nodes: \" + str(NX.number_of_nodes(nxg)))\n",
    "print (\"Number of Edges: \" + str(NX.number_of_edges(nxg)))\n",
    "print (\"Connected Components: \" + str(count))\n",
    "print (\"Runtime: \" + str((end_connected-start)))\n",
    "print (\"Graph Creation Runtime: \" + str((end_created-start)))\n",
    "print (\"Connected Components Runtime: \" + str((end_connected - end_created)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pilot_hadoop import PilotComputeService\n",
    "from IPython.display import HTML\n",
    "\n",
    "os.environ[\"SAGA_VERBOSE\"]=\"100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pilot-Spark and Pilot-InMemory Implementation\n",
    "\n",
    "Setup Spark cluster on local machine or HPC resource. Execute **either** 2.1.1 or 2.1.2\n",
    "\n",
    "### 2.1.1 Start Spark Cluster using Pilot-Spark\n",
    "\n",
    "see https://github.com/drelu/saga-hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pilot_compute_description = {\n",
    "                            \"resource_url\":\"fork://localhost\",\n",
    "                            \"number_cores\": 1,\n",
    "                      k      \"cores_per_node\":1,\n",
    "                            \"type\":\"spark\"\n",
    "                            }\n",
    "pilot = PilotComputeService.create_pilot(pilot_compute_description);\n",
    "\n",
    "# print out details of Pilot-Spark\n",
    "details = pilot.get_details()\n",
    "HTML(\"<a target='blank' href='%s'>Spark Web UI</a>\"%details[\"web_ui_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Start Spark Cluster inside YARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK Home: /usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6\n",
      "SPARK HOME: /usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6\n",
      "PYTHONPATH: /usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python:/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip\n"
     ]
    }
   ],
   "source": [
    "%run env.py\n",
    "%run util/init_spark.py\n",
    "\n",
    "from pilot_hadoop import PilotComputeService as PilotSparkComputeService\n",
    "\n",
    "pilotcompute_description = {\n",
    "    \"service_url\": \"yarn-client://yarn-aws.radical-cybertools.org\",\n",
    "    \"number_of_processes\": 16\n",
    "}\n",
    "\n",
    "print \"SPARK HOME: %s\"%os.environ[\"SPARK_HOME\"]\n",
    "print \"PYTHONPATH: %s\"%os.environ[\"PYTHONPATH\"]\n",
    "\n",
    "pilot_spark = PilotSparkComputeService.create_pilot(pilotcompute_description=pilotcompute_description)\n",
    "sc = pilot_spark.get_spark_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Utilize Spark Native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 luckow hdfs   29855041 2015-11-25 00:03 /data/mdanalysis/large/graph_edges_145746_1012872.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /data/mdanalysis/large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.version\n",
    "rdd = sc.textFile(\"/data/mdanalysis/large/graph_edges_145746_1012872.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012872"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Distance Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "coord = np.loadtxt(\"vesicle_1_5M_373_P_145746.np_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.linalg.distributed\n",
    "coord_matrix=pyspark.mllib.linalg.distributed.RowMatrix(sc.parallelize(coord, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd=coord_matrix.rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=row_rdd.sample(False, 0.01, 81)\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o108.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://radical-5:8020/user/luckow/Distances.csv already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1430)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1409)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:522)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:47)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-80602124f0e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdistances\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mrow_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcartesian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_rdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m            \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquared_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m            \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m15.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m            \u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Distances.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"ComputeDistance, %.2f\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1506\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1508\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/2.3.2.0-2950/spark-1.5.2-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o108.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://radical-5:8020/user/luckow/Distances.csv already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1430)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1409)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:522)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:47)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "distances=  row_rdd.cartesian(row_rdd).\\\n",
    "            map(lambda a: (a[0].squared_distance(a[1]))).\\\n",
    "            filter(lambda a: a>15.0).\\\n",
    "            saveAsTextFile(\"distances.csv\")\n",
    "print \"ComputeDistance, %.2f\"%(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "15/11/26 03:48:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\n",
      "Moved: 'hdfs://radical-5:8020/user/luckow/Distances.csv' to trash at: hdfs://radical-5:8020/user/luckow/.Trash/Current\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rmr Distances.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Leaflet Finder Pilot-InMemory Implementation (Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from distributed_inmem.dataunit_spark import DistributedInMemoryDataUnit\n",
    "import time\n",
    "\n",
    "FILENAME=\"../data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "\n",
    "FILENAME_ALL_EDGES=\"../data/mdanalysis/small/graph_edges_95_215_alledges.csv\"\n",
    "du = DistributedInMemoryDataUnit(name=\"LeafletFinderGraph\", sc=sc)\n",
    "\n",
    "#DistributedInMemoryDataUnit.spark_context.version\n",
    "\n",
    "f = open(FILENAME_ALL_EDGES)\n",
    "graph = f.readlines()\n",
    "du.load(graph)\n",
    "f.close()\n",
    "\n",
    "def identityMapper(edge, args):\n",
    "    #print edge\n",
    "    #comp = edge.strip().split(\",\")\n",
    "    #return (int(comp[0]), int(comp[1]))\n",
    "    return eval(str(edge))\n",
    "\n",
    "def groupByVertex(data):\n",
    "    print(\"Call reduce on: \" + str(data))\n",
    "    \n",
    "\n",
    "new_iteration_needed = du.sc.accumulator(0)\n",
    "\n",
    "# check for smaller keys in each set\n",
    "def process_vertex(vertex):\n",
    "    \"\"\" pass single vertex and its adjecent vertices\n",
    "        e.g.: (0, [0, 67, 14])\n",
    "    \"\"\"\n",
    "    global new_iteration_needed\n",
    "    vertex = eval(vertex)\n",
    "    source = int(vertex[0])\n",
    "    dest= sorted([int(i) for i in vertex[1]])\n",
    "    local_max = False\n",
    "    \n",
    "    first_edge_destination = int(dest[0])\n",
    "    new_vertices = []    \n",
    "    print \"*********Source: %d First Edge Dest: %d\"%(source, first_edge_destination) \n",
    "    if source <= first_edge_destination:\n",
    "        local_max = True\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "            \n",
    "\n",
    "    print \"Process: \" + str(vertex) + \" Local Max: \" + str(local_max)\n",
    "    last_edge_destination = first_edge_destination\n",
    "\n",
    "    for current_destination in vertex[1]:\n",
    "        print \"Current destination: %s\"%str(current_destination)\n",
    "        current_destination = int(current_destination)\n",
    "        if current_destination == last_edge_destination: \n",
    "            continue\n",
    "        \n",
    "        if local_max == True:\n",
    "            edge = (source, current_destination)\n",
    "            new_vertices.append(edge)\n",
    "        else:\n",
    "            new_vertices.append((first_edge_destination, current_destination))\n",
    "            new_vertices.append((current_destination, first_edge_destination))\n",
    "            print \"Add 1 to accumulator\"\n",
    "            new_iteration_needed.add(1)\n",
    "\n",
    "        last_edge_destination = current_destination\n",
    "    \n",
    "    if ((not local_max) and (source < last_edge_destination)):\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "    \n",
    "    print \"Return new vertices: \" + str(new_vertices)\n",
    "    return new_vertices\n",
    "\n",
    "\n",
    "#process_vertex(\"('19', ['19', '7', '9', '41'])\")\n",
    "num_iterations=0\n",
    "start = time.time()\n",
    "while True:\n",
    "    old_accum_value = new_iteration_needed.value\n",
    "    print \"*********** Start iteration: %d \" % num_iterations\n",
    "    future_result = du.map_pilot(identityMapper, None, number_of_compute_units=2)\n",
    "    result_du=future_result.result()[0]\n",
    "    future_result = result_du.reduce_pilot(process_vertex, number_of_compute_units=2)\n",
    "    output = future_result.result()\n",
    "    output.export()   \n",
    "    du = output\n",
    "    num_iterations = num_iterations + 1\n",
    "    print \"New iteration accum: %d old value: %d\"%(new_iteration_needed.value, old_accum_value)\n",
    "    if old_accum_value < new_iteration_needed.value:\n",
    "        #print \"Accumulator value was increased. New iteration.\"\n",
    "        continue        \n",
    "        #pass\n",
    "    else:\n",
    "        break\n",
    "    break\n",
    "end = time.time()\n",
    "print \"Final results: \"\n",
    "num_components=du.data.groupByKey().count()\n",
    "print \"Finished after %d Iterations. Found %d components. Time: %.2f\"%(num_iterations, num_components, (end-start)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "du.data.groupByKey().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Native Spark Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Load data from text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [0, 14, 67]), (2, [2, 13, 34, 62]), (4, [4, 27, 33, 68, 94]), (6, [6, 57, 64, 67]), (8, [8, 46, 69, 88, 93]), (10, [10, 31, 40, 71]), (12, [7, 12, 30, 50, 91]), (14, [0, 14, 48, 64]), (16, [16, 40]), (18, [18]), (20, [20, 85]), (22, [22, 54]), (24, [24, 62]), (26, [26, 49, 57, 70]), (28, [11, 17, 28, 69]), (30, [9, 12, 23, 30, 50]), (32, [32, 53]), (34, [2, 34]), (36, [36, 47, 84, 92]), (38, [9, 23, 38]), (40, [10, 16, 31, 40, 71]), (42, [42, 73]), (44, [44, 68]), (46, [8, 46]), (48, [14, 48, 49]), (50, [12, 23, 30, 50]), (52, [29, 52, 79, 89]), (54, [22, 54, 87]), (56, [56, 76]), (58, [58, 81]), (60, [17, 60, 69, 81]), (62, [2, 24, 25, 62, 79, 89]), (64, [6, 14, 57, 64, 67]), (66, [51, 66]), (68, [4, 44, 68, 72, 94]), (70, [26, 70, 79, 86]), (72, [68, 72]), (74, [43, 47, 53, 74, 82]), (76, [56, 76]), (78, [78, 92]), (80, [43, 80]), (82, [74, 77, 82]), (84, [36, 63, 84]), (86, [70, 79, 86]), (88, [8, 69, 88, 93]), (90, [11, 55, 75, 90]), (92, [36, 47, 78, 92]), (94, [3, 4, 33, 59, 68, 94]), (1, [1, 41]), (3, [3, 39, 59, 94]), (5, [5, 29, 37, 61]), (7, [7, 12, 19]), (9, [9, 19, 23, 30, 38]), (11, [11, 28, 55, 90, 93]), (13, [2, 13, 25]), (15, [15]), (17, [17, 28, 60, 69]), (19, [7, 9, 19, 41]), (21, [21]), (23, [9, 23, 30, 38, 50, 51]), (25, [13, 25, 61, 62, 89]), (27, [4, 27, 33]), (29, [5, 29, 52, 61, 65]), (31, [10, 31, 40]), (33, [4, 27, 33, 59, 91, 94]), (35, [35, 45, 63]), (37, [5, 37]), (39, [3, 39, 87]), (41, [1, 19, 41]), (43, [43, 47, 74, 80]), (45, [35, 45]), (47, [36, 43, 47, 53, 74, 92]), (49, [26, 48, 49]), (51, [23, 51, 66]), (53, [32, 47, 53, 74]), (55, [11, 55, 90, 93]), (57, [6, 26, 57, 64]), (59, [3, 33, 59, 91, 94]), (61, [5, 25, 29, 61, 89]), (63, [35, 63, 84]), (65, [29, 65]), (67, [0, 6, 64, 67]), (69, [8, 17, 28, 60, 69, 88]), (71, [10, 40, 71]), (73, [42, 73, 81]), (75, [75, 85, 90]), (77, [77, 82]), (79, [52, 62, 70, 79, 86, 89]), (81, [58, 60, 73, 81]), (83, [83]), (85, [20, 75, 85]), (87, [39, 54, 87]), (89, [25, 52, 61, 62, 79, 89]), (91, [12, 33, 59, 91]), (93, [8, 11, 55, 88, 93])]\n",
      "CPU times: user 12 ms, sys: 743 µs, total: 12.7 ms\n",
      "Wall time: 680 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FILENAME=\"/data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "data = sc.textFile(FILENAME).map(lambda line: [int(i) for i in line.split(\",\")])\n",
    "# add backward edges\n",
    "data = data.flatMap(lambda v: [(v[0],v[1]),(v[1],v[0])])\n",
    "\n",
    "#data.saveAsTextFile(\"../data/mdanalysis/small/graph_edges_95_215_alledges.csv\")\n",
    "#data = data.filter(lambda v: v[0] != v[1])\n",
    "#print data.collect()\n",
    "\n",
    "data_grouped = data.groupByKey().mapValues(lambda a: sorted(set(a)))\n",
    "print data_grouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Connected Component Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Start iteration: 0 \n",
      "*********** Start iteration: 1 \n",
      "*********** Start iteration: 2 \n",
      "*********** Start iteration: 3 \n",
      "*********** Start iteration: 4 \n",
      "*********** Start iteration: 5 \n",
      "Finished after 6 Iterations. Found 10 components. Time: 2.39\n"
     ]
    }
   ],
   "source": [
    "new_iteration_needed = sc.accumulator(0)\n",
    "# check for smaller keys in each set\n",
    "def process_vertex(vertex):\n",
    "    \"\"\" pass single vertex and its adjecent vertices\n",
    "        e.g.: (0, [0, 67, 14])\n",
    "    \"\"\"\n",
    "    global new_iteration_needed\n",
    "    source = vertex[0]\n",
    "    local_max = False\n",
    "    \n",
    "    first_edge_destination = vertex[1][0]\n",
    "    new_vertices = []    \n",
    "    print \"*********Source: %d First Edge Dest: %d\"%(source, first_edge_destination) \n",
    "    if source <= first_edge_destination:\n",
    "        local_max = True\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "            \n",
    "    #pdb.set_trace()\n",
    "    print \"Process: \" + str(vertex) + \" Local Max: \" + str(local_max)\n",
    "    last_edge_destination = first_edge_destination\n",
    "\n",
    "    #if vertex[1]==None or len(vertex[1])<=1:\n",
    "    #    new_vertices.append((source, source))   \n",
    "    for current_destination in vertex[1]:\n",
    "        #print \"Current destination: %s\"%str(current_destination)\n",
    "        if current_destination == last_edge_destination: \n",
    "            continue\n",
    "        \n",
    "        if local_max == True:\n",
    "            edge = (source, current_destination)\n",
    "            new_vertices.append(edge)\n",
    "        else:\n",
    "            new_vertices.append((first_edge_destination, current_destination))\n",
    "            new_vertices.append((current_destination, first_edge_destination))\n",
    "            print \"Add 1 to accumulator\"\n",
    "            new_iteration_needed.add(1)\n",
    "\n",
    "        last_edge_destination = current_destination\n",
    "    \n",
    "    if ((not local_max) and (source < last_edge_destination)):\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "    \n",
    "    #print \"Return new vertices: \" + str(new_vertices)\n",
    "    return new_vertices\n",
    "\n",
    "\n",
    "#process_vertex((19, [7, 9, 19, 41]))\n",
    "num_iterations=0\n",
    "cc = data_grouped\n",
    "start = time.time()\n",
    "while True:\n",
    "    old_accum_value = new_iteration_needed.value\n",
    "    print \"*********** Start iteration: %d \" % num_iterations\n",
    "    #print \"Accum before iteration: \" + str(old_accum_value)\n",
    "    cc = cc.flatMap(lambda v: process_vertex(v))\\\n",
    "           .groupByKey()\\\n",
    "           .mapValues(lambda a: sorted(set(a)))\n",
    "    cc.collect()\n",
    "    num_iterations = num_iterations + 1\n",
    "    #print \"New iteration accum: %d old value: %d\"%(new_iteration_needed.value, old_accum_value)\n",
    "    if old_accum_value < new_iteration_needed.value:\n",
    "        #print \"Accumulator value was increased. New iteration.\"\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "end = time.time()\n",
    "\n",
    "print \"Finished after %d Iterations. Found %d components. Time: %.2f\"%(num_iterations, cc.count(), (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Scratch Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start = df_grouped\n",
    "result=start.flatMap(lambda v: (v[0], v[1])).map(lambda v: v<start_index).countByValue()\n",
    "\n",
    "print sttrresult\n",
    "\n",
    "local_max = not result.has_key(True)\n",
    "\n",
    "print \"Local Max: \" + str(local_max) + \" Smaller Index: \" + str(result.has_key(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "            StructField(\"source\", IntegerType(), True),\n",
    "            StructField(\"destination\", IntegerType(), True)\n",
    "        ])\n",
    "df = sqlCtx.createDataFrame(data, schema)\n",
    "df.explain()\n",
    "schema_grouped = StructType([\n",
    "            StructField(\"source\", IntegerType(), True),\n",
    "            StructField(\"destination\", ArrayType(IntegerType()), True)\n",
    "        ])\n",
    "df_grouped = sqlCtx.createDataFrame(data_grouped, schema_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "t = udf(lambda s: str(s), StringType())\n",
    "slen = udf(lambda s: Column(len(s)), IntegerType())\n",
    "\n",
    "#df.groupBy(\"source\").collect()\n",
    "#df.groupBy(\"source\").agg(df.source, t(df.source))\n",
    "\n",
    "c = df.groupBy(df.source).agg(col(\"source\"), slen(df.destination))\n",
    "\n",
    "#c = df.agg(col(\"source\"), t(df.destination).alias('counts'))\n",
    "c.head(5)\n",
    "\n",
    "\n",
    "#c = df.groupBy(df.source).lit(df.destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertices = df.select(df[\"source\"]).unionAll(df.select(df[\"destination\"]))\n",
    "vertices = di_source.distinct()\n",
    "\n",
    "print \"Number of vertices: %d\"%(vertices.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphlab import SGraph, SFrame\n",
    "from graphlab import connected_components\n",
    "\n",
    "PROBLEM={\"small\": \"./data/mdanalysis/small/graph_edges_95_215.csv\",\n",
    "         \"medium\":\"./data/mdanalysis/medium/graph_edges_24056_71826.csv\"}\n",
    "\n",
    "d =datetime.datetime.now()\n",
    "RESULTSFILE = \"results-\" + d.strftime(\"%Y%m%d-%H%M%S\") + \".csv\"\n",
    "REPEATS=5\n",
    "\n",
    "start = time.time()\n",
    "data = SFrame.read_csv(filename, header=False)\n",
    "sg = SGraph().add_edges(data, src_field=\"X1\", dst_field=\"X2\")\n",
    "end_read=time.time()\n",
    "cc = connected_components.create(sg)\n",
    "s=cc[\"component_size\"]\n",
    "end_connected = time.time()\n",
    "print cc\n",
    "print s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
