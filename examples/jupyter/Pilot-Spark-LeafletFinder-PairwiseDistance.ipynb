{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDAnalysis and Pilot-In-Memory (Pairwise Distance)\n",
    "\n",
    "\n",
    "The main performance bottleneck of the current MDAnalysis implementation is the construction of the graph using NetworkX taking ~78% of the overall runtime.\n",
    "\n",
    "\n",
    "**Beckstein Profiling:**\n",
    "\n",
    "    47        10           33      3.3      0.0      if adj is None:\n",
    "    48        10        66544   6654.4      0.0          x = atoms.positions\n",
    "\n",
    "    54        10     58689221 5868922.1     18.8          adj = (MDAnalysis.core.parallel.distances.distance_array(x, x, box=box) < cutoff)\n",
    "    \n",
    "    58        10           78      7.8      0.0      adjk = adj if Nmax is None else adj[:Nmax, :Nmax] \n",
    "    59        10    243009076 24300907.6   77.9      graph = nx.Graph(adjk)\n",
    "    60        10      4346636 434663.6      1.4      subgraphs = nx.connected_components(graph)\n",
    "    61        49        83597   1706.1      0.0      indices = [np.sort(g) for g in subgraphs]\n",
    "    62        49      5694698 116218.3      1.8      return [atoms[group].residues for group in indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LeafletFinder NetworkX Implementation Profiling\n",
    "\n",
    "see https://code.google.com/p/mdanalysis/\n",
    "\n",
    "Profile default implementation based on [NetworkX](https://networkx.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "FILENAME=\"../data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "!head -n 5 {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%time\n",
    "import networkx as NX\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "start = time.time()\n",
    "nxg = NX.read_edgelist(FILENAME, delimiter=\",\")\n",
    "end_read = time.time()\n",
    "NX.draw(nxg, pos=NX.spring_layout(nxg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "degree_sequence=sorted(NX.degree(nxg).values(),reverse=True) # degree sequence\n",
    "print \"Degree sequence\", degree_sequence\n",
    "print \"Length: %d\" % len(degree_sequence)\n",
    "\n",
    "dmax=max(degree_sequence)\n",
    "\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Degree Histogram\")\n",
    "plt.ylabel(\"Degree\")\n",
    "plt.xlabel(\"Node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "components = NX.connected_components(nxg)\n",
    "end_created = time.time()\n",
    "count = 0\n",
    "for component in components:\n",
    "    print str(sorted(component))\n",
    "    count = count + 1\n",
    "end_connected = time.time()\n",
    "print (\"Number of Nodes: \" + str(NX.number_of_nodes(nxg)))\n",
    "print (\"Number of Edges: \" + str(NX.number_of_edges(nxg)))\n",
    "print (\"Connected Components: \" + str(count))\n",
    "print (\"Runtime: \" + str((end_connected-start)))\n",
    "print (\"Graph Creation Runtime: \" + str((end_created-start)))\n",
    "print (\"Connected Components Runtime: \" + str((end_connected - end_created)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pilot_hadoop import PilotComputeService\n",
    "from IPython.display import HTML\n",
    "\n",
    "os.environ[\"SAGA_VERBOSE\"]=\"100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pilot-Spark and Pilot-InMemory Implementation\n",
    "\n",
    "Setup Spark cluster on local machine or HPC resource. Execute **either** 2.1.1 or 2.1.2\n",
    "\n",
    "### Start Spark Cluster using Pilot-Spark (Stampede)\n",
    "\n",
    "see https://github.com/drelu/saga-hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pilot_compute_description = {\n",
    "                            \"resource_url\":\"fork://localhost\",\n",
    "                            \"number_cores\": 1,\n",
    "                            \"cores_per_node\":1,\n",
    "                            \"type\":\"spark\"\n",
    "                            }\n",
    "pilot = PilotComputeService.create_pilot(pilot_compute_description);\n",
    "\n",
    "# print out details of Pilot-Spark\n",
    "details = pilot.get_details()\n",
    "HTML(\"<a target='blank' href='%s'>Spark Web UI</a>\"%details[\"web_ui_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pilot_compute_description = {\n",
    "                            \"resource_url\":\"spark://129.114.58.102:7077\",\n",
    "                            \"type\":\"spark\"\n",
    "                            }\n",
    "pilot = PilotComputeService.create_pilot(pilot_compute_description);\n",
    "\n",
    "# print out details of Pilot-Spark\n",
    "details = pilot.get_details()\n",
    "HTML(\"<a target='blank' href='%s'>Spark Web UI</a>\"%details[\"web_ui_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Start Spark Cluster inside YARN (Chameleon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%run util/init_spark.py\n",
    "\n",
    "NUMBER_EXECUTORS=10\n",
    "\n",
    "from pilot_hadoop import PilotComputeService as PilotSparkComputeService\n",
    "\n",
    "pilotcompute_description = {\n",
    "    \"service_url\": \"yarn-client://yarn.radical-cybertools.org\",\n",
    "    \"number_of_processes\": NUMBER_EXECUTORS,\n",
    "    \"physical_memory_per_process\": \"16G\" \n",
    "}\n",
    "\n",
    "print \"SPARK HOME: %s\"%os.environ[\"SPARK_HOME\"]\n",
    "print \"PYTHONPATH: %s\"%os.environ[\"PYTHONPATH\"]\n",
    "\n",
    "start = time.time()\n",
    "pilot_spark = PilotSparkComputeService.create_pilot(pilotcompute_description=pilotcompute_description)\n",
    "sc = pilot_spark.get_spark_context()\n",
    "print \"Spark Startup, %.2f\"%(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3. Start Spark Cluster (Wrangler)\n",
    "\n",
    "    export JAVA_HOME=/usr/java/jdk1.8.0_45/\n",
    "    saga-hadoop --resource=slurm://localhost --queue=normal --walltime=59 --number_cores=24 --project=TG-MCB090174 --framework spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%run util/init_spark_wrangler.py\n",
    "\n",
    "from pilot_hadoop import PilotComputeService as PilotSparkComputeService\n",
    "\n",
    "pilotcompute_description = {\n",
    "    \"service_url\": \"spark://129.114.58.105:7077\"\n",
    "}\n",
    "\n",
    "print \"SPARK HOME: %s\"%os.environ[\"SPARK_HOME\"]\n",
    "print \"PYTHONPATH: %s\"%os.environ[\"PYTHONPATH\"]\n",
    "\n",
    "start = time.time()\n",
    "pilot_spark = PilotSparkComputeService.create_pilot(pilotcompute_description=pilotcompute_description)\n",
    "sc = pilot_spark.get_spark_context()\n",
    "print \"Spark Startup, %.2f\"%(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Spark Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!hadoop fs -text \"/data/mdanalysis/synthetic/10.np_txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sc.addFile(\"hdfs:///data/mdanalysis/synthetic/10.np_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "rdd=sc.parallelize(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "rdd.map(lambda a: subprocess.check_output('python --version', shell=True, stderr=subprocess.STDOUT)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Computation\n",
    "\n",
    "## Use File Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "from MDAnalysis.core.distances import distance_array, self_distance_array\n",
    "from MDAnalysis.analysis.distances import contact_matrix\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "cutoff = 15.0\n",
    "\n",
    "def get_edges_partition(adjacency_matrix, cutoff=15.0):\n",
    "    it = np.nditer(adjacency_matrix, flags=['multi_index'])\n",
    "    edge_list = []\n",
    "    while not it.finished:\n",
    "        value = it[0]\n",
    "        if cutoff < value:\n",
    "            # only connect 1 undirectional edge, e.g. <0,1>, but not <1,0>'\n",
    "            if it.multi_index[0]<=it.multi_index[1]:\n",
    "                edge_list.append((it.multi_index[0], it.multi_index[1]))\n",
    "                #print \"%d <%s>\" % (it[0], it.multi_index),\n",
    "        it.iternext()\n",
    "    return edge_list\n",
    "\n",
    "def compute_distance_file(point_index_data_file):\n",
    "    # 1-D Partitioning\n",
    "    point_index_start = point_index_data_file[0]\n",
    "    point_index_end = point_index_data_file[1]\n",
    "    data = point_index_data_file[2]\n",
    "    filename=SparkFiles.get(data)\n",
    "    coord_all = np.loadtxt(filename, dtype='float32')\n",
    "    coord_part = coord_all[point_index_start:point_index_end]\n",
    "    adj = cdist(coord_part, coord_all)\n",
    "    edge_list = get_edges_partition(adj)\n",
    "    del adj\n",
    "    #del coord_part\n",
    "    #del coord_all\n",
    "    #gc.collect()\n",
    "    #return edge_list\n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read File Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "#data=sc.textFile(DATA_FILE).collect()\n",
    "import hdfs\n",
    "import numpy as np\n",
    "client = hdfs.client.Client(\"http://radical-5:50070\")\n",
    "DATA_FILE=\"hdfs:///data/mdanalysis/small/md_centered.xtc_95Atoms.np_txt\"\n",
    "PARTITION_SIZE=10\n",
    "content=None\n",
    "with client.read(DATA_FILE.replace(\"hdfs://\", \"\")) as reader:\n",
    "    content=reader.read()\n",
    "data=np.fromstring(content, dtype=\"float32\", sep=\" \\n\")\n",
    "data=data.reshape(len(data)/3,3)\n",
    "number_rows=len(data)\n",
    "number_partitions=(number_rows/PARTITION_SIZE)+1\n",
    "print \"Number Partitions: %d\"%number_partitions\n",
    "\n",
    "partitions=map(lambda a: (a*PARTITION_SIZE, \n",
    "                         (a*PARTITION_SIZE)+PARTITION_SIZE, \n",
    "                         os.path.basename(DATA_FILE)), \n",
    "           range(number_partitions))\n",
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "len(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sc.addFile(DATA_FILE)\n",
    "part_rdd=sc.parallelize(partitions, len(partitions))\n",
    "start = time.time()\n",
    "edges_list=part_rdd.map(compute_distance_file).flatMap(lambda a: a).collect()\n",
    "print str(len(edges_list))\n",
    "print \"ComputeDistanceSpark, %d, %d, %.2f\"%(len(data), NUMBER_EXECUTORS, (time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import hdfs\n",
    "import numpy as np\n",
    "client = hdfs.client.Client(\"http://radical-5:50070\")\n",
    "content=client.read(\"/data/mdanalysis/small/md_centered.xtc_95Atoms.np_txt\").read()\n",
    "data_np=np.fromstring(content, dtype=\"float32\", sep=\" \\n\")\n",
    "data_np=data_np.reshape(len(data_np)/3,3)\n",
    "len(data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_np=np.fromstring(content, dtype=\"float32\", sep=\" \\n\")\n",
    "data_np=data_np.reshape(len(data_np)/3,3)\n",
    "len(data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_EXECUTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "os.path.basename(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast-based Implementation (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#coord = np.loadtxt(\"vesicle_1_5M_373_stride1000.xtc_145746Atoms.np_txt\", dtype='float32')\n",
    "#coord = np.loadtxt(\"/data/mdanalysis/medium/md_prod_12x12_everymicroS_pbcmolcenter.xtc_44784Atoms.np_txt\", dtype='float32')\n",
    "DATA_PATH=\"../data/mdanalysis/synthetic/traj/\"\n",
    "files=[os.path.join(DATA_PATH, i) for i in os.listdir(DATA_PATH) if i.endswith(\".np_txt\")]\n",
    "NUMBER_EXECUTORS=48\n",
    "\n",
    "#coord = np.loadtxt(\"/data/mdanalysis/small/md_centered.xtc_95Atoms.np_txt\", dtype='float32')\n",
    "coord = np.loadtxt(files[-1], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls /data/mdanalysis/small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coord_broadcast = sc.broadcast(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coord_all = coord_broadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coord_all[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "part_rdd=sc.parallelize(range(len(coord_broadcast.value)), NUMBER_EXECUTORS)\n",
    "part_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code wraps the MDAnalysis functions into Spark code that is executed in a data-parallel way either on an individual or a batch of points (1-D partitioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from MDAnalysis.core.distances import distance_array, self_distance_array\n",
    "from MDAnalysis.analysis.distances import contact_matrix\n",
    "import scipy.sparse\n",
    "from scipy.spatial.distance import cdist\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "\n",
    "cutoff = 15.0\n",
    "\n",
    "################################################################################\n",
    "# Process batch of points (a partition of the RDD)\n",
    "\n",
    "def get_edges_partition(adjacency_matrix, cutoff=15.0):\n",
    "    it = np.nditer(adjacency_matrix, flags=['multi_index'])\n",
    "    edge_list = []\n",
    "    while not it.finished:\n",
    "        value = it[0]\n",
    "        if cutoff < value:\n",
    "            # only connect 1 undirectional edge, e.g. <0,1>, but not <1,0>'\n",
    "            if it.multi_index[0]<=it.multi_index[1]:\n",
    "                edge_list.append((it.multi_index[0], it.multi_index[1]))\n",
    "                #print \"%d <%s>\" % (it[0], it.multi_index),\n",
    "        it.iternext()\n",
    "    return edge_list\n",
    "\n",
    "def compute_distance_partition(iterator):\n",
    "    \"\"\"Partition points in 1-D\"\"\"\n",
    "    min_value=sys.maxint\n",
    "    max_value=-sys.maxint-1\n",
    "    for i in iterator:\n",
    "        if i < min_value:\n",
    "            min_value = i\n",
    "        if i > max_value:\n",
    "            max_value = i\n",
    "    \n",
    "    # 2-D Partitioning\n",
    "    coord_all = coord_broadcast.value\n",
    "    coord_part = coord_all[min_value:max_value]\n",
    "    #print \"**All**\"\n",
    "    #print str(coord_all)\n",
    "    #print \"**Part**\"\n",
    "    #print str(coord_part)\n",
    "    #adj=contact_matrix(coord_part, returntype=\"sparse\")\n",
    "    #adj = distance_array(coord_part, coord_all, box=None)\n",
    "    adj = cdist(coord_part, coord_all)\n",
    "    #print \"**scipy.spatial.distance.cdist**\"\n",
    "    #print(adj)\n",
    "    #adj2 = distance_array(coord_part, coord_all, box=None)\n",
    "    #print \"**MDAnalysis**\"\n",
    "    #print(adj2)\n",
    "    \n",
    "    edge_list = get_edges_partition(adj)\n",
    "    del coord_part\n",
    "    del coord_all\n",
    "    del adj\n",
    "    gc.collect()\n",
    "    return edge_list\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Process one point at a time\n",
    "def get_edges_point(point_index, adjacency_matrix, cutoff=15.0):\n",
    "    edge_list = []\n",
    "    for index, i in np.ndenumerate(adjacency_matrix):\n",
    "        #print (\"Index: %d, Value: %d\"%(index[i], i))\n",
    "        #if point_index<=index[1] and i<cutoff:\n",
    "        if i==True and point_index<=index[1]:\n",
    "            # Attention we only compute the upper half of the adjacency matrix\n",
    "            # thus we need to offset the target edge vertice by point_index\n",
    "            edge_list.append((point_index, point_index+index[1]))\n",
    "    #del adjacency_matrix\n",
    "    return edge_list\n",
    "\n",
    "def compute_distance(point_index):\n",
    "    # 1-D Partitioning\n",
    "    coord_all = coord_broadcast.value\n",
    "    coord_part = coord_all[point_index-1:point_index]\n",
    "    #adj = (distance_array(coord_part, coord_all[point_index:], box=None) < cutoff)\n",
    "    adj = (cdist(coord_part, coord_all) < cutoff)\n",
    "    #adj = cdist(coord_part, coord_all)\n",
    "    edge_list = get_edges_point(point_index, adj)\n",
    "    del adj\n",
    "    #del coord_part\n",
    "    #del coord_all\n",
    "    #gc.collect()\n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#edges_list=part_rdd.map(compute_distance).flatMap(lambda a: a).collect()\n",
    "edges_list_spark=part_rdd.mapPartitions(compute_distance_partition).collect()\n",
    "print str(len(edges_list))\n",
    "print \"ComputeDistanceSpark, %d, %d, %.2f\"%(len(coord_all), NUMBER_EXECUTORS, (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "#edges_list_local = compute_distance_partition(iter(range(20000)))\n",
    "#print str(len(edges_list))\n",
    "#print \"ComputeDistanceLocal, %d, %.2f\"%(len(coord_all),(time.time()-start))\n",
    "\n",
    "for i in range(1):\n",
    "    start = time.time()\n",
    "    edges_list=part_rdd.map(compute_distance).flatMap(lambda a: a).collect()\n",
    "    #edges_list_spark=part_rdd.mapPartitions(compute_distance_partition).collect()\n",
    "    print str(len(edges_list))\n",
    "    print \"ComputeDistanceSpark, %d, %d, %.2f\"%(len(coord_all), NUMBER_EXECUTORS, (time.time()-start))\n",
    "    del edges_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unoptimized using cartesian product\n",
    "\n",
    "Not good for sparse result data. Only usable on a very small sample:\n",
    "\n",
    "    sample=row_rdd.sample(False, 0.01, 81)\n",
    "    sample.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.linalg.distributed\n",
    "coord_matrix=pyspark.mllib.linalg.distributed.RowMatrix(sc.parallelize(coord[:200], 4))\n",
    "row_rdd=coord_matrix.rows\n",
    "row_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "row_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coord = np.loadtxt(\"md_centered.xtc_95Atoms.np_txt\", dtype='float32')\n",
    "#coord_str = np.array2string(coord, separator=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coord.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coord_str=[]\n",
    "for i in range(len(coord)):\n",
    "    coord_str.append(str(coord[i][0]) +\",\"+ str(coord[i][1]) +\",\"+ str(coord[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "sc.parallelize(range(200), 4).count()\n",
    "print \"Count, %.2f\"%((time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "row_rdd.cartesian(row_rdd).map(lambda a: a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "distances=  row_rdd.cartesian(row_rdd).\\\n",
    "            map(lambda a: (a[0].squared_distance(a[1]))).\\\n",
    "            filter(lambda a: a>15.0).\\\n",
    "            saveAsTextFile(\"distances.csv\")\n",
    "print \"ComputeDistance, %.2f\"%(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-D Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_partitions=4\n",
    "\n",
    "def compute_distance_2d(partition_index):\n",
    "    # 2-D Partitioning\n",
    "    coord_all = coord_broadcast.value[:100]\n",
    "    length = len(coord_all)\n",
    "    # identify square to work on    \n",
    "    xdim = math.sqrt(num_partitions)\n",
    "    ydim = math.sqrt(num_partitions)\n",
    "    xdim/partition_index\n",
    "    len=len(coord_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connected Components: Pilot-InMemory Implementation (Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from distributed_inmem.dataunit_spark import DistributedInMemoryDataUnit\n",
    "import time\n",
    "\n",
    "FILENAME=\"../data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "\n",
    "FILENAME_ALL_EDGES=\"../data/mdanalysis/small/graph_edges_95_215_alledges.csv\"\n",
    "du = DistributedInMemoryDataUnit(name=\"LeafletFinderGraph\", sc=sc)\n",
    "\n",
    "#DistributedInMemoryDataUnit.spark_context.version\n",
    "\n",
    "f = open(FILENAME_ALL_EDGES)\n",
    "graph = f.readlines()\n",
    "du.load(graph)\n",
    "f.close()\n",
    "\n",
    "def identityMapper(edge, args):\n",
    "    #print edge\n",
    "    #comp = edge.strip().split(\",\")\n",
    "    #return (int(comp[0]), int(comp[1]))\n",
    "    return eval(str(edge))\n",
    "\n",
    "def groupByVertex(data):\n",
    "    print(\"Call reduce on: \" + str(data))\n",
    "    \n",
    "\n",
    "new_iteration_needed = du.sc.accumulator(0)\n",
    "\n",
    "# check for smaller keys in each set\n",
    "def process_vertex(vertex):\n",
    "    \"\"\" pass single vertex and its adjecent vertices\n",
    "        e.g.: (0, [0, 67, 14])\n",
    "    \"\"\"\n",
    "    global new_iteration_needed\n",
    "    vertex = eval(vertex)\n",
    "    source = int(vertex[0])\n",
    "    dest= sorted([int(i) for i in vertex[1]])\n",
    "    local_max = False\n",
    "    \n",
    "    first_edge_destination = int(dest[0])\n",
    "    new_vertices = []    \n",
    "    print \"*********Source: %d First Edge Dest: %d\"%(source, first_edge_destination) \n",
    "    if source <= first_edge_destination:\n",
    "        local_max = True\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "            \n",
    "\n",
    "    print \"Process: \" + str(vertex) + \" Local Max: \" + str(local_max)\n",
    "    last_edge_destination = first_edge_destination\n",
    "\n",
    "    for current_destination in vertex[1]:\n",
    "        print \"Current destination: %s\"%str(current_destination)\n",
    "        current_destination = int(current_destination)\n",
    "        if current_destination == last_edge_destination: \n",
    "            continue\n",
    "        \n",
    "        if local_max == True:\n",
    "            edge = (source, current_destination)\n",
    "            new_vertices.append(edge)\n",
    "        else:\n",
    "            new_vertices.append((first_edge_destination, current_destination))\n",
    "            new_vertices.append((current_destination, first_edge_destination))\n",
    "            print \"Add 1 to accumulator\"\n",
    "            new_iteration_needed.add(1)\n",
    "\n",
    "        last_edge_destination = current_destination\n",
    "    \n",
    "    if ((not local_max) and (source < last_edge_destination)):\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "    \n",
    "    print \"Return new vertices: \" + str(new_vertices)\n",
    "    return new_vertices\n",
    "\n",
    "\n",
    "#process_vertex(\"('19', ['19', '7', '9', '41'])\")\n",
    "num_iterations=0\n",
    "start = time.time()\n",
    "while True:\n",
    "    old_accum_value = new_iteration_needed.value\n",
    "    print \"*********** Start iteration: %d \" % num_iterations\n",
    "    future_result = du.map_pilot(identityMapper, None, number_of_compute_units=2)\n",
    "    result_du=future_result.result()[0]\n",
    "    future_result = result_du.reduce_pilot(process_vertex, number_of_compute_units=2)\n",
    "    output = future_result.result()\n",
    "    output.export()   \n",
    "    du = output\n",
    "    num_iterations = num_iterations + 1\n",
    "    print \"New iteration accum: %d old value: %d\"%(new_iteration_needed.value, old_accum_value)\n",
    "    if old_accum_value < new_iteration_needed.value:\n",
    "        #print \"Accumulator value was increased. New iteration.\"\n",
    "        continue        \n",
    "        #pass\n",
    "    else:\n",
    "        break\n",
    "    break\n",
    "end = time.time()\n",
    "print \"Final results: \"\n",
    "num_components=du.data.groupByKey().count()\n",
    "print \"Finished after %d Iterations. Found %d components. Time: %.2f\"%(num_iterations, num_components, (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "du.data.groupByKey().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Native Spark Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls /data/mdanalysis/large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/mdanalysis/large/graph_edges_145746_1012872.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Load data from text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "FILENAME=\"/data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "data = sc.textFile(FILENAME).map(lambda line: [int(i) for i in line.split(\",\")])\n",
    "# add backward edges\n",
    "data = data.flatMap(lambda v: [(v[0],v[1]),(v[1],v[0])])\n",
    "\n",
    "#data.saveAsTextFile(\"../data/mdanalysis/small/graph_edges_95_215_alledges.csv\")\n",
    "#data = data.filter(lambda v: v[0] != v[1])\n",
    "#print data.collect()\n",
    "\n",
    "data_grouped = data.groupByKey().mapValues(lambda a: sorted(set(a)))\n",
    "print data_grouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connected Component Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "new_iteration_needed = sc.accumulator(0)\n",
    "# check for smaller keys in each set\n",
    "def process_vertex(vertex):\n",
    "    \"\"\" pass single vertex and its adjecent vertices\n",
    "        e.g.: (0, [0, 67, 14])\n",
    "    \"\"\"\n",
    "    global new_iteration_needed\n",
    "    source = vertex[0]\n",
    "    local_max = False\n",
    "    \n",
    "    first_edge_destination = vertex[1][0]\n",
    "    new_vertices = []    \n",
    "    print \"*********Source: %d First Edge Dest: %d\"%(source, first_edge_destination) \n",
    "    if source <= first_edge_destination:\n",
    "        local_max = True\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "            \n",
    "    #pdb.set_trace()\n",
    "    print \"Process: \" + str(vertex) + \" Local Max: \" + str(local_max)\n",
    "    last_edge_destination = first_edge_destination\n",
    "\n",
    "    #if vertex[1]==None or len(vertex[1])<=1:\n",
    "    #    new_vertices.append((source, source))   \n",
    "    for current_destination in vertex[1]:\n",
    "        # print \"Current destination: %s\"%str(current_destination)\n",
    "        # remove duplicates\n",
    "        if current_destination == last_edge_destination: \n",
    "            continue\n",
    "        \n",
    "        if local_max == True:\n",
    "            edge = (source, current_destination)\n",
    "            new_vertices.append(edge)\n",
    "        else:\n",
    "            new_vertices.append((first_edge_destination, current_destination))\n",
    "            new_vertices.append((current_destination, first_edge_destination))\n",
    "            print \"Add 1 to accumulator\"\n",
    "            new_iteration_needed.add(1)\n",
    "\n",
    "        last_edge_destination = current_destination\n",
    "    \n",
    "    if ((not local_max) and (source < last_edge_destination)):\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "    \n",
    "    #print \"Return new vertices: \" + str(new_vertices)\n",
    "    return new_vertices\n",
    "\n",
    "\n",
    "#process_vertex((19, [7, 9, 19, 41]))\n",
    "num_iterations=0\n",
    "cc = data_grouped\n",
    "start = time.time()\n",
    "while True:\n",
    "    old_accum_value = new_iteration_needed.value\n",
    "    print \"*********** Start iteration: %d \" % num_iterations\n",
    "    #print \"Accum before iteration: \" + str(old_accum_value)\n",
    "    cc = cc.flatMap(lambda v: process_vertex(v))\\\n",
    "           .groupByKey()\\\n",
    "           .mapValues(lambda a: sorted(set(a)))\n",
    "    cc.collect()\n",
    "    num_iterations = num_iterations + 1\n",
    "    #print \"New iteration accum: %d old value: %d\"%(new_iteration_needed.value, old_accum_value)\n",
    "    if old_accum_value < new_iteration_needed.value:\n",
    "        #print \"Accumulator value was increased. New iteration.\"\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "end = time.time()\n",
    "\n",
    "print \"Finished after %d Iterations. Found %d components. Time: %.2f\"%(num_iterations, cc.count(), (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-26T14:54:05.477596",
     "start_time": "2016-12-26T14:54:05.472186"
    }
   },
   "source": [
    "# Giannis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def get_distance(Atom1, Atom2):\n",
    "    # Calculate Euclidean distance. 1-D and 3-D in the future\n",
    "    return np.sqrt(sum((Atom1 - Atom2) ** 2))\n",
    "\n",
    "def n_dim_input_to_numpy_array(temp):\n",
    "    temp = temp.split(',')\n",
    "    temp = map(float,temp)\n",
    "    return np.asfarray(temp)\n",
    "    calc_count = calc_count +1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    args = sys.argv[1:]\n",
    "    WINDOW_SIZE = int(sys.argv[1])\n",
    "    reading_start_point_i = int(sys.argv[2]) -1\n",
    "    j_dim = int(sys.argv[3]) -1\n",
    "    total_file_lines =  int(sys.argv[4])\n",
    "    cutoff = float(sys.argv[5])\n",
    "\n",
    "    #----------------------Reading Input File-------------------------------#\n",
    "\n",
    "    read_file = open('input.txt')\n",
    "\n",
    "\n",
    "    atoms = list()\n",
    "    for count, line in enumerate(read_file):\n",
    "        if count == total_file_lines+1 or count >= reading_start_point_i+WINDOW_SIZE:\n",
    "            break\n",
    "        if count >= reading_start_point_i and count <reading_start_point_i+WINDOW_SIZE:\n",
    "            atoms.append(n_dim_input_to_numpy_array(line))\n",
    "\n",
    "    # That means that we are not calculating the elements of the main diagonal which are the same.\n",
    "    # We do calculate differnt atoms\n",
    "    if reading_start_point_i!=j_dim:\n",
    "        atomsY = list()\n",
    "        atomsY.append(n_dim_input_to_numpy_array(line)) # already read from previous for-loop\n",
    "        for countY, line in enumerate(read_file):\n",
    "            if countY > j_dim + WINDOW_SIZE-count-1:\n",
    "                break\n",
    "            if countY >= j_dim-count and countY < j_dim + WINDOW_SIZE-count-1:  #-1 because we already appended the first line \n",
    "                atomsY.append(n_dim_input_to_numpy_array(line))\n",
    "    read_file.close()\n",
    "\n",
    "    # the difference is that in the Cus compute data that are in main diagonal compute half of the elements \n",
    "    # because table is symmetric, so the second loop can be half in the first case \n",
    "    distances=np.empty((WINDOW_SIZE,WINDOW_SIZE),dtype='bool')\n",
    "    if reading_start_point_i == j_dim:\n",
    "        for i in range(0,WINDOW_SIZE):\n",
    "            for j in range(i+1,WINDOW_SIZE):\n",
    "                dist = get_distance(atoms[i],atoms[j])  \n",
    "                if dist<=cutoff:\n",
    "                    distances[i][j]=True \n",
    "                else:\n",
    "                    distances[i][j]=False\n",
    "    else:\n",
    "        for i in range(0,WINDOW_SIZE):\n",
    "            for j in range(0,WINDOW_SIZE):\n",
    "                dist = get_distance(atoms[i],atomsY[j])  \n",
    "                if dist<=cutoff:\n",
    "                    distances[i][j]=True\n",
    "                else:\n",
    "                    distances[i][j]=False\n",
    "\n",
    "    np.save(\"distances_%d_%d.npz.npy\" % (reading_start_point_i,j_dim),distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scratch Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#start = df_grouped\n",
    "result=start.flatMap(lambda v: (v[0], v[1])).map(lambda v: v<start_index).countByValue()\n",
    "\n",
    "print sttrresult\n",
    "\n",
    "local_max = not result.has_key(True)\n",
    "\n",
    "print \"Local Max: \" + str(local_max) + \" Smaller Index: \" + str(result.has_key(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "            StructField(\"source\", IntegerType(), True),\n",
    "            StructField(\"destination\", IntegerType(), True)\n",
    "        ])\n",
    "df = sqlCtx.createDataFrame(data, schema)\n",
    "df.explain()\n",
    "schema_grouped = StructType([\n",
    "            StructField(\"source\", IntegerType(), True),\n",
    "            StructField(\"destination\", ArrayType(IntegerType()), True)\n",
    "        ])\n",
    "df_grouped = sqlCtx.createDataFrame(data_grouped, schema_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "t = udf(lambda s: str(s), StringType())\n",
    "slen = udf(lambda s: Column(len(s)), IntegerType())\n",
    "\n",
    "#df.groupBy(\"source\").collect()\n",
    "#df.groupBy(\"source\").agg(df.source, t(df.source))\n",
    "\n",
    "c = df.groupBy(df.source).agg(col(\"source\"), slen(df.destination))\n",
    "\n",
    "#c = df.agg(col(\"source\"), t(df.destination).alias('counts'))\n",
    "c.head(5)\n",
    "\n",
    "\n",
    "#c = df.groupBy(df.source).lit(df.destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "vertices = df.select(df[\"source\"]).unionAll(df.select(df[\"destination\"]))\n",
    "vertices = di_source.distinct()\n",
    "\n",
    "print \"Number of vertices: %d\"%(vertices.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from graphlab import SGraph, SFrame\n",
    "from graphlab import connected_components\n",
    "\n",
    "PROBLEM={\"small\": \"./data/mdanalysis/small/graph_edges_95_215.csv\",\n",
    "         \"medium\":\"./data/mdanalysis/medium/graph_edges_24056_71826.csv\"}\n",
    "\n",
    "d =datetime.datetime.now()\n",
    "RESULTSFILE = \"results-\" + d.strftime(\"%Y%m%d-%H%M%S\") + \".csv\"\n",
    "REPEATS=5\n",
    "\n",
    "start = time.time()\n",
    "data = SFrame.read_csv(filename, header=False)\n",
    "sg = SGraph().add_edges(data, src_field=\"X1\", dst_field=\"X2\")\n",
    "end_read=time.time()\n",
    "cc = connected_components.create(sg)\n",
    "s=cc[\"component_size\"]\n",
    "end_connected = time.time()\n",
    "print cc\n",
    "print s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "407px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_section_display": "block",
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "toc_position": {
   "height": "876px",
   "left": "9.43182px",
   "right": "20px",
   "top": "105px",
   "width": "221px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
