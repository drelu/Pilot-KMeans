{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PyKafka Streaming Tests\n",
    "\n",
    "Manual Start without SAGA-Hadoop\n",
    "\n",
    "    zookeeper-server-start.sh -daemon kafka-fe7e1dc0-32bb-11e7-8f5f-549f350759f8/config/zookeeper.properties\n",
    "    kafka-server-start.sh -daemon kafka-fe7e1dc0-32bb-11e7-8f5f-549f350759f8/config/broker-0/server.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from pykafka import KafkaClient\n",
    "import numpy as np\n",
    "#import msgpack\n",
    "#import msgpack_numpy as m\n",
    "#m.patch()\n",
    "\n",
    "zkKafka='c251-109.wrangler.tacc.utexas.edu:2181'\n",
    "client = KafkaClient(zookeeper_hosts=zkKafka)\n",
    "#client = KafkaClient(hosts='c251-142.wrangler.tacc.utexas.edu:9092')\n",
    "topic = client.topics['latency']\n",
    "producer = topic.get_sync_producer()\n",
    "consumer = topic.get_simple_consumer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <pykafka.broker.Broker at 0x2b5158d23d90 (host=c251-109, port=9092, id=0)>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.brokers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_CLUSTER=2\n",
    "NUMBER_POINTS_PER_CLUSTER=10\n",
    "NUMBER_DIM=3\n",
    "\n",
    "def get_random_cluster_points(number_points, number_dim):\n",
    "    mu = np.random.randn()\n",
    "    sigma = np.random.randn()\n",
    "    p = sigma * np.random.randn(number_points, number_dim) + mu\n",
    "    return p\n",
    "\n",
    "\n",
    "points = []\n",
    "for i in range(NUMBER_CLUSTER):    \n",
    "    p = get_random_cluster_points(NUMBER_POINTS_PER_CLUSTER, NUMBER_DIM)\n",
    "    points.append(p)\n",
    "    \n",
    "points_np=np.concatenate(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "points_strlist=str(points_np.tolist())\n",
    "producer.produce(points_strlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#points_msg=msgpack.packb(points_np, default=m.encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-17T13:30:54.497918\n"
     ]
    }
   ],
   "source": [
    "ts = datetime.datetime.now().isoformat() \n",
    "producer.produce(ts + \";1\")        \n",
    "print ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent @ 2016-12-17T14:18:17.832688\n",
      "sent @ 2016-12-17T14:18:18.947443\n",
      "sent @ 2016-12-17T14:18:19.951786\n",
      "sent @ 2016-12-17T14:18:20.968272\n",
      "sent @ 2016-12-17T14:18:21.984779\n",
      "sent @ 2016-12-17T14:18:23.001421\n",
      "sent @ 2016-12-17T14:18:24.017938\n",
      "sent @ 2016-12-17T14:18:25.034441\n",
      "sent @ 2016-12-17T14:18:26.199190\n",
      "sent @ 2016-12-17T14:18:27.313965\n",
      "sent @ 2016-12-17T14:18:28.428757\n",
      "sent @ 2016-12-17T14:18:29.543467\n",
      "sent @ 2016-12-17T14:18:30.658202\n",
      "sent @ 2016-12-17T14:18:31.772929\n",
      "sent @ 2016-12-17T14:18:32.887653\n",
      "sent @ 2016-12-17T14:18:34.002390\n",
      "sent @ 2016-12-17T14:18:35.117119\n",
      "sent @ 2016-12-17T14:18:36.231843\n",
      "sent @ 2016-12-17T14:18:37.346586\n",
      "sent @ 2016-12-17T14:18:38.354986\n",
      "sent @ 2016-12-17T14:18:39.469821\n",
      "sent @ 2016-12-17T14:18:39.683581\n",
      "sent @ 2016-12-17T14:18:39.897419\n",
      "sent @ 2016-12-17T14:18:40.111290\n",
      "sent @ 2016-12-17T14:18:40.325188\n",
      "sent @ 2016-12-17T14:18:40.539044\n",
      "sent @ 2016-12-17T14:18:40.702835\n",
      "sent @ 2016-12-17T14:18:40.916687\n",
      "sent @ 2016-12-17T14:18:41.130573\n",
      "sent @ 2016-12-17T14:18:41.344469\n",
      "sent @ 2016-12-17T14:18:41.558337\n",
      "sent @ 2016-12-17T14:18:41.722133\n",
      "sent @ 2016-12-17T14:18:41.935851\n",
      "sent @ 2016-12-17T14:18:42.149748\n",
      "sent @ 2016-12-17T14:18:42.363683\n",
      "sent @ 2016-12-17T14:18:42.577575\n",
      "sent @ 2016-12-17T14:18:42.741367\n",
      "sent @ 2016-12-17T14:18:42.955222\n",
      "sent @ 2016-12-17T14:18:43.169121\n",
      "sent @ 2016-12-17T14:18:43.382999\n",
      "sent @ 2016-12-17T14:18:43.596902\n",
      "sent @ 2016-12-17T14:18:43.670664\n",
      "sent @ 2016-12-17T14:18:43.794412\n",
      "sent @ 2016-12-17T14:18:43.868149\n",
      "sent @ 2016-12-17T14:18:43.991984\n",
      "sent @ 2016-12-17T14:18:44.115796\n",
      "sent @ 2016-12-17T14:18:44.189554\n",
      "sent @ 2016-12-17T14:18:44.313372\n",
      "sent @ 2016-12-17T14:18:44.387063\n",
      "sent @ 2016-12-17T14:18:44.510929\n",
      "sent @ 2016-12-17T14:18:44.634745\n",
      "sent @ 2016-12-17T14:18:44.708505\n",
      "sent @ 2016-12-17T14:18:44.832319\n",
      "sent @ 2016-12-17T14:18:44.906040\n",
      "sent @ 2016-12-17T14:18:45.029893\n",
      "sent @ 2016-12-17T14:18:45.103626\n",
      "sent @ 2016-12-17T14:18:45.227445\n",
      "sent @ 2016-12-17T14:18:45.351281\n",
      "sent @ 2016-12-17T14:18:45.425028\n",
      "sent @ 2016-12-17T14:18:45.548859\n",
      "sent @ 2016-12-17T14:18:45.622605\n",
      "sent @ 2016-12-17T14:18:45.737404\n",
      "sent @ 2016-12-17T14:18:45.852197\n",
      "sent @ 2016-12-17T14:18:45.967029\n",
      "sent @ 2016-12-17T14:18:46.031757\n",
      "sent @ 2016-12-17T14:18:46.146574\n",
      "sent @ 2016-12-17T14:18:46.261423\n",
      "sent @ 2016-12-17T14:18:46.376231\n",
      "sent @ 2016-12-17T14:18:46.440983\n",
      "sent @ 2016-12-17T14:18:46.555790\n",
      "sent @ 2016-12-17T14:18:46.670592\n",
      "sent @ 2016-12-17T14:18:46.785390\n",
      "sent @ 2016-12-17T14:18:46.900186\n",
      "sent @ 2016-12-17T14:18:46.964890\n",
      "sent @ 2016-12-17T14:18:47.079660\n",
      "sent @ 2016-12-17T14:18:47.194458\n",
      "sent @ 2016-12-17T14:18:47.309288\n",
      "sent @ 2016-12-17T14:18:47.373987\n",
      "sent @ 2016-12-17T14:18:47.488778\n",
      "sent @ 2016-12-17T14:18:47.603572\n",
      "sent @ 2016-12-17T14:18:47.718318\n",
      "sent @ 2016-12-17T14:18:47.782106\n",
      "sent @ 2016-12-17T14:18:47.895987\n",
      "sent @ 2016-12-17T14:18:48.009864\n",
      "sent @ 2016-12-17T14:18:48.123732\n",
      "sent @ 2016-12-17T14:18:48.237575\n",
      "sent @ 2016-12-17T14:18:48.301358\n",
      "sent @ 2016-12-17T14:18:48.415213\n",
      "sent @ 2016-12-17T14:18:48.529086\n",
      "sent @ 2016-12-17T14:18:48.642997\n",
      "sent @ 2016-12-17T14:18:48.706793\n",
      "sent @ 2016-12-17T14:18:48.820641\n",
      "sent @ 2016-12-17T14:18:48.934564\n",
      "sent @ 2016-12-17T14:18:49.048449\n",
      "sent @ 2016-12-17T14:18:49.162355\n",
      "sent @ 2016-12-17T14:18:49.226143\n",
      "sent @ 2016-12-17T14:18:49.340021\n",
      "sent @ 2016-12-17T14:18:49.453953\n",
      "sent @ 2016-12-17T14:18:49.567842\n",
      "sent @ 2016-12-17T14:18:49.631625\n",
      "sent @ 2016-12-17T14:18:49.745512\n",
      "sent @ 2016-12-17T14:18:49.859262\n",
      "sent @ 2016-12-17T14:18:49.973070\n",
      "sent @ 2016-12-17T14:18:50.086796\n",
      "sent @ 2016-12-17T14:18:50.150507\n",
      "sent @ 2016-12-17T14:18:50.264319\n",
      "sent @ 2016-12-17T14:18:50.378068\n",
      "sent @ 2016-12-17T14:18:50.491877\n",
      "sent @ 2016-12-17T14:18:50.555538\n",
      "sent @ 2016-12-17T14:18:50.669272\n",
      "sent @ 2016-12-17T14:18:50.783107\n",
      "sent @ 2016-12-17T14:18:50.896857\n",
      "sent @ 2016-12-17T14:18:51.010671\n",
      "sent @ 2016-12-17T14:18:51.074414\n",
      "sent @ 2016-12-17T14:18:51.188177\n",
      "sent @ 2016-12-17T14:18:51.302076\n",
      "sent @ 2016-12-17T14:18:51.415869\n",
      "sent @ 2016-12-17T14:18:51.479620\n",
      "sent @ 2016-12-17T14:18:51.593377\n",
      "sent @ 2016-12-17T14:18:51.707130\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "#sent_time=datetime.datetime.now().isoformat()\n",
    "sleep_time = 1\n",
    "topic = client.topics['latency']\n",
    "producer = topic.get_sync_producer()\n",
    "\n",
    "for sleep_time in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "    for i in range(20):\n",
    "        producer.produce(datetime.datetime.now().isoformat() + \";\" + str(sleep_time))\n",
    "        print \"sent @ \" + datetime.datetime.now().isoformat()\n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named py4j.java_gateway",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/home/01131/tg804093/notebooks/Pilot-Memory/examples/jupyter/util/init_spark_wrangler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# import Spark Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAccumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAccumulatorParam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/01131/tg804093/work/spark-2.0.2-bin-hadoop2.6/python/pyspark/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/01131/tg804093/work/spark-2.0.2-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_gateway\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPickleSerializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUTF8Deserializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mPairDeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoBatchedSerializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoOpSerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/01131/tg804093/work/spark-2.0.2-bin-hadoop2.6/python/pyspark/java_gateway.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mxrange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_gateway\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjava_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJavaGateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGatewayClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_collections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mListConverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named py4j.java_gateway"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name accumulators",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bcd54dbfd0b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'run ../util/init_spark_wrangler.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/01131/tg804093/work/spark-2.0.2-bin-hadoop2.6/python/pyspark/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/01131/tg804093/work/spark-2.0.2-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtempfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccumulators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name accumulators"
     ]
    }
   ],
   "source": [
    "%run ../util/init_spark_wrangler.py\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "points_np=np.array(points_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "for i in points_np:\n",
    "     v = Vectors.dense(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "v=Vectors.dense(points_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "x_rec = msgpack.unpackb(x_enc, object_hook=m.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "b=points_np.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "type(pickle.loads(serialized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
