{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import numpy\n",
    "from optparse import OptionParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOG_DIR='/work/01131/tg804093/wrangler/spark-logs'\n",
    "LOG_NAME=os.path.join(LOG_DIR, \"app-20170805180556-0003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Event\":\"SparkListenerLogStart\",\"Spark Version\":\"2.2.0\"}\r\n",
      "{\"Event\":\"SparkListenerBlockManagerAdded\",\"Block Manager ID\":{\"Executor ID\":\"driver\",\"Host\":\"129.114.58.107\",\"Port\":42546},\"Maximum Memory\":384093388,\"Timestamp\":1501974356267,\"Maximum Onheap Memory\":384093388,\"Maximum Offheap Memory\":0}\r\n",
      "{\"Event\":\"SparkListenerEnvironmentUpdate\",\"JVM Information\":{\"Java Home\":\"/usr/java/jdk1.8.0_92/jre\",\"Java Version\":\"1.8.0_92 (Oracle Corporation)\",\"Scala Version\":\"version 2.11.8\"},\"Spark Properties\":{\"spark.driver.host\":\"129.114.58.107\",\"spark.serializer.objectStreamReset\":\"100\",\"spark.eventLog.enabled\":\"true\",\"spark.driver.port\":\"47534\",\"spark.rdd.compress\":\"True\",\"spark.jars\":\"file:/home/01131/tg804093/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.0.2.jar,file:/home/01131/tg804093/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar,file:/home/01131/tg804093/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.0.2.jar,file:/home/01131/tg804093/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:/home/01131/tg804093/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar,file:/home/01131/tg804093/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar,file:/home/01131/tg804093/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.2.jar,file:/home/01131/tg804093/.ivy2/jars/com.101tec_zkclient-0.3.jar,file:/home/01131/tg804093/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar,file:/home/01131/tg804093/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar,file:/home/01131/tg804093/.ivy2/jars/log4j_log4j-1.2.17.jar,file:/home/01131/tg804093/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar,file:/home/01131/tg804093/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar,file:/home/01131/tg804093/.ivy2/jars/org.scalatest_scalatest_2.11-2.2.6.jar,file:/home/01131/tg804093/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar\",\"spark.app.name\":\"Pilot-Spark\",\"spark.scheduler.mode\":\"FIFO\",\"spark.submit.pyFiles\":\"/home/01131/tg804093/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.0.2.jar,/home/01131/tg804093/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar,/home/01131/tg804093/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.0.2.jar,/home/01131/tg804093/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/home/01131/tg804093/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar,/home/01131/tg804093/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar,/home/01131/tg804093/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.2.jar,/home/01131/tg804093/.ivy2/jars/com.101tec_zkclient-0.3.jar,/home/01131/tg804093/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar,/home/01131/tg804093/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar,/home/01131/tg804093/.ivy2/jars/log4j_log4j-1.2.17.jar,/home/01131/tg804093/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar,/home/01131/tg804093/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar,/home/01131/tg804093/.ivy2/jars/org.scalatest_scalatest_2.11-2.2.6.jar,/home/01131/tg804093/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar\",\"spark.files\":\"file:/home/01131/tg804093/notebooks/Pilot-Memory/examples/jupyter/streaming/throughput/../saga_hadoop_utils.py,file:/home/01131/tg804093/notebooks/Pilot-Memory/examples/jupyter/streaming/throughput/KMeans_SparkStreamingThroughputConsumer.py,file:/home/01131/tg804093/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.0.2.jar,file:/home/01131/tg804093/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar,file:/home/01131/tg804093/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.0.2.jar,file:/home/01131/tg804093/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:/home/01131/tg804093/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar,file:/home/01131/tg804093/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar,file:/home/01131/tg804093/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.2.jar,file:/home/01131/tg804093/.ivy2/jars/com.101tec_zkclient-0.3.jar,file:/home/01131/tg804093/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar,file:/home/01131/tg804093/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar,file:/home/01131/tg804093/.ivy2/jars/log4j_log4j-1.2.17.jar,file:/home/01131/tg804093/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar,file:/home/01131/tg804093/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar,file:/home/01131/tg804093/.ivy2/jars/org.scalatest_scalatest_2.11-2.2.6.jar,file:/home/01131/tg804093/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar\",\"spark.executor.id\":\"driver\",\"spark.submit.deployMode\":\"client\",\"spark.master\":\"spark://129.114.58.115:7077\",\"spark.eventLog.dir\":\"file:///work/01131/tg804093/wrangler/spark-logs\",\"spark.app.id\":\"app-20170805180556-0003\"},\"System Properties\":{\"java.io.tmpdir\":\"/tmp\",\"line.separator\":\"\\n\",\"path.separator\":\":\",\"sun.management.compiler\":\"HotSpot 64-Bit Tiered Compilers\",\"SPARK_SUBMIT\":\"true\",\"sun.cpu.endian\":\"little\",\"java.specification.version\":\"1.8\",\"java.vm.specification.name\":\"Java Virtual Machine Specification\",\"java.vendor\":\"Oracle Corporation\",\"java.vm.specification.version\":\"1.8\",\"user.home\":\"/home/01131/tg804093\",\"file.encoding.pkg\":\"sun.io\",\"sun.nio.ch.bugLevel\":\"\",\"sun.arch.data.model\":\"64\",\"sun.boot.library.path\":\"/usr/java/jdk1.8.0_92/jre/lib/amd64\",\"user.dir\":\"/home/01131/tg804093/notebooks/Pilot-Memory/examples/jupyter/streaming/throughput\",\"java.library.path\":\"/opt/apps/intel15/mvapich2/2.1/lib:/opt/apps/intel15/mvapich2/2.1/lib/shared:/opt/apps/intel/15/composer_xe_2015.3.187/mpirt/lib/intel64:/opt/apps/intel/15/composer_xe_2015.3.187/ipp/lib/intel64:/opt/apps/intel/15/composer_xe_2015.3.187/mkl/lib/intel64:/opt/apps/intel/15/composer_xe_2015.3.187/tbb/lib/intel64:/opt/apps/intel/15/composer_xe_2015.3.187/tbb/lib/intel64/gcc4.4:/opt/apps/intel/15/composer_xe_2015.3.187/compiler/lib/intel64:/opt/apps/gcc/4.9.1/lib:/opt/apps/gcc/4.9.1/lib64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\",\"sun.cpu.isalist\":\"\",\"os.arch\":\"amd64\",\"java.vm.version\":\"25.92-b14\",\"java.endorsed.dirs\":\"/usr/java/jdk1.8.0_92/jre/lib/endorsed\",\"java.runtime.version\":\"1.8.0_92-b14\",\"java.vm.info\":\"mixed mode\",\"java.ext.dirs\":\"/usr/java/jdk1.8.0_92/jre/lib/ext:/usr/java/packages/lib/ext\",\"java.runtime.name\":\"Java(TM) SE Runtime Environment\",\"file.separator\":\"/\",\"java.class.version\":\"52.0\",\"java.specification.name\":\"Java Platform API Specification\",\"sun.boot.class.path\":\"/usr/java/jdk1.8.0_92/jre/lib/resources.jar:/usr/java/jdk1.8.0_92/jre/lib/rt.jar:/usr/java/jdk1.8.0_92/jre/lib/sunrsasign.jar:/usr/java/jdk1.8.0_92/jre/lib/jsse.jar:/usr/java/jdk1.8.0_92/jre/lib/jce.jar:/usr/java/jdk1.8.0_92/jre/lib/charsets.jar:/usr/java/jdk1.8.0_92/jre/lib/jfr.jar:/usr/java/jdk1.8.0_92/jre/classes\",\"file.encoding\":\"UTF-8\",\"user.timezone\":\"America/Chicago\",\"java.specification.vendor\":\"Oracle Corporation\",\"sun.java.launcher\":\"SUN_STANDARD\",\"os.version\":\"3.10.0-327.4.4.el7.x86_64\",\"sun.os.patch.level\":\"unknown\",\"java.vm.specification.vendor\":\"Oracle Corporation\",\"user.country\":\"US\",\"sun.jnu.encoding\":\"UTF-8\",\"user.language\":\"en\",\"java.vendor.url\":\"http://java.oracle.com/\",\"java.awt.printerjob\":\"sun.print.PSPrinterJob\",\"java.awt.graphicsenv\":\"sun.awt.X11GraphicsEnvironment\",\"awt.toolkit\":\"sun.awt.X11.XToolkit\",\"os.name\":\"Linux\",\"java.vm.vendor\":\"Oracle Corporation\",\"java.vendor.url.bug\":\"http://bugreport.sun.com/bugreport/\",\"user.name\":\"tg804093\",\"java.vm.name\":\"Java HotSpot(TM) 64-Bit Server VM\",\"sun.java.command\":\"org.apache.spark.deploy.SparkSubmit --conf spark.eventLog.enabled=true --conf spark.eventLog.dir=file:///work/01131/tg804093/wrangler/spark-logs --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 --files ../saga_hadoop_utils.py KMeans_SparkStreamingThroughputConsumer.py\",\"java.home\":\"/usr/java/jdk1.8.0_92/jre\",\"java.version\":\"1.8.0_92\",\"sun.io.unicode.encoding\":\"UnicodeLittle\"},\"Classpath Entries\":{\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/metrics-json-3.1.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/curator-client-2.6.0.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/org.scala-lang_scala-reflect-2.11.8.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.0.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/chill-java-0.8.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/mesos-1.0.0-shaded-protobuf.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-codec-1.10.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/activation-1.1.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/curator-framework-2.6.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/chill_2.11-0.8.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jsp-api-2.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.2.11.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jta-1.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/lz4-1.3.0.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/org.spark-project.spark_unused-1.0.0.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/scala-compiler-2.11.8.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/paranamer-2.6.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/metrics-core-3.1.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/parquet-jackson-1.8.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/py4j-0.10.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/gson-2.2.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/curator-recipes-2.6.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/kryo-shaded-3.0.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.2.0.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/pyrolite-4.13.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-sql_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/guava-14.0.1.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/net.jpountz.lz4_lz4-1.3.0.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-compress-1.4.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/scala-reflect-2.11.8.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/org.apache.spark_spark-tags_2.11-2.0.2.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/javolution-5.5.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-net-2.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/scala-library-2.11.8.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/json4s-core_2.11-3.2.11.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/snappy-java-1.1.2.6.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/org.scalatest_scalatest_2.11-2.2.6.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/guice-3.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/java-xmlbuilder-1.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/org.scala-lang_scala-reflect-2.11.8.jar\":\"Added By User\",\"spark://129.114.58.107:47534/files/org.apache.spark_spark-tags_2.11-2.0.2.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-core_2.11-2.2.0.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/com.yammer.metrics_metrics-core-2.2.0.jar\":\"Added By User\",\"spark://129.114.58.107:47534/jars/org.scalatest_scalatest_2.11-2.2.6.jar\":\"Added By User\",\"spark://129.114.58.107:47534/files/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.0.2.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-compiler-3.0.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jets3t-0.9.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/xmlenc-0.52.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/avro-1.7.7.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/net.jpountz.lz4_lz4-1.3.0.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/avro-ipc-1.7.7.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.5.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/httpcore-4.4.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-lang-2.6.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/bcprov-jdk15on-1.51.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.6.5.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-cli-1.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/pmml-schema-1.2.15.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.2.11.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/metrics-graphite-3.1.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/univocity-parsers-2.2.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/minlog-1.3.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/mail-1.4.7.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/log4j_log4j-1.2.17.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jpam-1.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jline-2.12.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/metrics-jvm-3.1.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jackson-annotations-2.6.5.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/xbean-asm5-shaded-4.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-repl_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-1.8.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jackson-core-2.6.5.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/parquet-column-1.8.2.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/com.101tec_zkclient-0.3.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/ivy-2.4.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jackson-databind-2.6.5.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-hive_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/com.101tec_zkclient-0.3.jar\":\"Added By User\",\"spark://129.114.58.107:47534/files/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.2.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.2.0.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.0.2.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/parquet-format-2.3.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/stream-2.7.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/opencsv-2.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/parquet-encoding-1.8.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.2.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/avro-mapred-1.7.7-hadoop2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/log4j-1.2.17.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/mx4j-3.0.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/ST4-4.0.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.2.0.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/org.apache.kafka_kafka-clients-0.8.2.1.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-tags_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/snappy-0.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-digester-1.8.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/breeze_2.11-0.13.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/janino-3.0.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/httpclient-4.5.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/xz-1.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/pmml-model-1.2.15.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/scalap-2.11.8.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/javax.inject-1.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/org.slf4j_slf4j-api-1.7.16.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-io-2.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/oro-2.0.8.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jetty-6.1.26.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/antlr4-runtime-4.5.3.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/org.xerial.snappy_snappy-java-1.1.2.6.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/antlr-2.7.7.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/org.apache.kafka_kafka_2.11-0.8.2.1.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/KMeans_SparkStreamingThroughputConsumer.py\":\"Added By User\",\"spark://129.114.58.107:47534/files/org.slf4j_slf4j-api-1.7.16.jar\":\"Added By User\",\"spark://129.114.58.107:47534/files/saga_hadoop_utils.py\":\"Added By User\",\"spark://129.114.58.107:47534/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/conf/\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/objenesis-2.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/netty-all-4.0.43.Final.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/parquet-common-1.8.2.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/base64-2.3.8.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/com.yammer.metrics_metrics-core-2.2.0.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/jars/org.spark-project.spark_unused-1.0.0.jar\":\"Added By User\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.2.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/core-1.1.2.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/aopalliance-1.0.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\":\"System Classpath\",\"/home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\":\"System Classpath\",\"spark://129.114.58.107:47534/files/log4j_log4j-1.2.17.jar\":\"Added By User\"}}\r\n",
      "{\"Event\":\"SparkListenerApplicationStart\",\"App Name\":\"Pilot-Spark\",\"App ID\":\"app-20170805180556-0003\",\"Timestamp\":1501974355174,\"User\":\"tg804093\"}\r\n",
      "{\"Event\":\"SparkListenerExecutorAdded\",\"Timestamp\":1501974357692,\"Executor ID\":\"0\",\"Executor Info\":{\"Host\":\"129.114.58.115\",\"Total Cores\":48,\"Log Urls\":{\"stdout\":\"http://129.114.58.115:8081/logPage/?appId=app-20170805180556-0003&executorId=0&logType=stdout\",\"stderr\":\"http://129.114.58.115:8081/logPage/?appId=app-20170805180556-0003&executorId=0&logType=stderr\"}}}\r\n",
      "{\"Event\":\"SparkListenerBlockManagerAdded\",\"Block Manager ID\":{\"Executor ID\":\"0\",\"Host\":\"129.114.58.115\",\"Port\":33749},\"Maximum Memory\":384093388,\"Timestamp\":1501974357728,\"Maximum Onheap Memory\":384093388,\"Maximum Offheap Memory\":0}\r\n",
      "{\"Event\":\"SparkListenerJobStart\",\"Job ID\":0,\"Submission Time\":1501974360148,\"Stage Infos\":[{\"Stage ID\":0,\"Stage Attempt ID\":0,\"Stage Name\":\"call at /home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:2230\",\"Number of Tasks\":48,\"RDD Info\":[{\"RDD ID\":2,\"Name\":\"PythonRDD\",\"Callsite\":\"call at /home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:2230\",\"Parent IDs\":[1],\"Storage Level\":{\"Use Disk\":false,\"Use Memory\":false,\"Deserialized\":false,\"Replication\":1},\"Number of Partitions\":48,\"Number of Cached Partitions\":0,\"Memory Size\":0,\"Disk Size\":0},{\"RDD ID\":0,\"Name\":\"KafkaRDD\",\"Scope\":\"{\\\"id\\\":\\\"0_1501974360000\\\",\\\"name\\\":\\\"kafka direct stream [0]\\\\n@ 18:06:00\\\"}\",\"Callsite\":\"createDirectStreamWithoutMessageHandler at NativeMethodAccessorImpl.java:0\",\"Parent IDs\":[],\"Storage Level\":{\"Use Disk\":false,\"Use Memory\":false,\"Deserialized\":false,\"Replication\":1},\"Number of Partitions\":48,\"Number of Cached Partitions\":0,\"Memory Size\":0,\"Disk Size\":0},{\"RDD ID\":1,\"Name\":\"PythonRDD\",\"Callsite\":\"RDD at PythonRDD.scala:48\",\"Parent IDs\":[0],\"Storage Level\":{\"Use Disk\":false,\"Use Memory\":false,\"Deserialized\":false,\"Replication\":1},\"Number of Partitions\":48,\"Number of Cached Partitions\":0,\"Memory Size\":0,\"Disk Size\":0}],\"Parent IDs\":[],\"Details\":\"org.apache.spark.rdd.RDD.collect(RDD.scala:935)\\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\njava.lang.reflect.Method.invoke(Method.java:498)\\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\npy4j.Gateway.invoke(Gateway.java:280)\\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\npy4j.commands.CallCommand.execute(CallCommand.java:79)\\npy4j.GatewayConnection.run(GatewayConnection.java:214)\\njava.lang.Thread.run(Thread.java:745)\",\"Accumulables\":[]}],\"Stage IDs\":[0],\"Properties\":{\"spark.rdd.scope.noOverride\":\"true\",\"spark.rdd.scope\":\"{\\\"id\\\":\\\"2\\\",\\\"name\\\":\\\"collect\\\"}\",\"callSite.short\":\"call at /home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:2230\"}}\r\n",
      "{\"Event\":\"SparkListenerStageSubmitted\",\"Stage Info\":{\"Stage ID\":0,\"Stage Attempt ID\":0,\"Stage Name\":\"call at /home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:2230\",\"Number of Tasks\":48,\"RDD Info\":[{\"RDD ID\":2,\"Name\":\"PythonRDD\",\"Callsite\":\"call at /home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:2230\",\"Parent IDs\":[1],\"Storage Level\":{\"Use Disk\":false,\"Use Memory\":false,\"Deserialized\":false,\"Replication\":1},\"Number of Partitions\":48,\"Number of Cached Partitions\":0,\"Memory Size\":0,\"Disk Size\":0},{\"RDD ID\":0,\"Name\":\"KafkaRDD\",\"Scope\":\"{\\\"id\\\":\\\"0_1501974360000\\\",\\\"name\\\":\\\"kafka direct stream [0]\\\\n@ 18:06:00\\\"}\",\"Callsite\":\"createDirectStreamWithoutMessageHandler at NativeMethodAccessorImpl.java:0\",\"Parent IDs\":[],\"Storage Level\":{\"Use Disk\":false,\"Use Memory\":false,\"Deserialized\":false,\"Replication\":1},\"Number of Partitions\":48,\"Number of Cached Partitions\":0,\"Memory Size\":0,\"Disk Size\":0},{\"RDD ID\":1,\"Name\":\"PythonRDD\",\"Callsite\":\"RDD at PythonRDD.scala:48\",\"Parent IDs\":[0],\"Storage Level\":{\"Use Disk\":false,\"Use Memory\":false,\"Deserialized\":false,\"Replication\":1},\"Number of Partitions\":48,\"Number of Cached Partitions\":0,\"Memory Size\":0,\"Disk Size\":0}],\"Parent IDs\":[],\"Details\":\"org.apache.spark.rdd.RDD.collect(RDD.scala:935)\\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\njava.lang.reflect.Method.invoke(Method.java:498)\\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\npy4j.Gateway.invoke(Gateway.java:280)\\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\npy4j.commands.CallCommand.execute(CallCommand.java:79)\\npy4j.GatewayConnection.run(GatewayConnection.java:214)\\njava.lang.Thread.run(Thread.java:745)\",\"Accumulables\":[]},\"Properties\":{\"spark.rdd.scope.noOverride\":\"true\",\"spark.rdd.scope\":\"{\\\"id\\\":\\\"2\\\",\\\"name\\\":\\\"collect\\\"}\",\"callSite.short\":\"call at /home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:2230\"}}\r\n",
      "{\"Event\":\"SparkListenerTaskStart\",\"Stage ID\":0,\"Stage Attempt ID\":0,\"Task Info\":{\"Task ID\":0,\"Index\":0,\"Attempt\":0,\"Launch Time\":1501974360244,\"Executor ID\":\"0\",\"Host\":\"129.114.58.115\",\"Locality\":\"ANY\",\"Speculative\":false,\"Getting Result Time\":0,\"Finish Time\":0,\"Failed\":false,\"Killed\":false,\"Accumulables\":[]}}\r\n",
      "{\"Event\":\"SparkListenerTaskStart\",\"Stage ID\":0,\"Stage Attempt ID\":0,\"Task Info\":{\"Task ID\":1,\"Index\":1,\"Attempt\":0,\"Launch Time\":1501974360253,\"Executor ID\":\"0\",\"Host\":\"129.114.58.115\",\"Locality\":\"ANY\",\"Speculative\":false,\"Getting Result Time\":0,\"Finish Time\":0,\"Failed\":false,\"Killed\":false,\"Accumulables\":[]}}\r\n"
     ]
    }
   ],
   "source": [
    "!head {LOG_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(LOG_NAME, \"r\")\n",
    "test_line = f.readline()\n",
    "log = json.loads(test_line.strip(\"\\n\").replace(\"\\n\", \"\\\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Event': u'SparkListenerLogStart', u'Spark Version': u'2.2.0'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48,3533.00\n",
      "48,3138.00\n",
      "48,1082.00\n",
      "48,4079.00\n",
      "48,89.00\n",
      "48,1152.00\n",
      "48,1295.00\n",
      "48,38.00\n",
      "48,985.00\n",
      "48,1250.00\n",
      "48,1497.00\n",
      "48,46.00\n",
      "48,922.00\n",
      "48,1179.00\n",
      "48,1330.00\n",
      "48,91.00\n",
      "48,1184.00\n",
      "48,1586.00\n",
      "48,1747.00\n",
      "48,55.00\n",
      "48,798.00\n",
      "48,1152.00\n",
      "48,1128.00\n",
      "48,28.00\n",
      "48,771.00\n",
      "48,1030.00\n",
      "48,1237.00\n",
      "48,29.00\n",
      "48,919.00\n",
      "48,1392.00\n",
      "48,1409.00\n",
      "48,34.00\n",
      "48,980.00\n",
      "48,1295.00\n",
      "48,1474.00\n",
      "48,25.00\n",
      "48,1205.00\n",
      "48,1450.00\n",
      "48,1641.00\n",
      "48,24.00\n",
      "48,972.00\n",
      "48,1427.00\n",
      "48,1426.00\n",
      "48,24.00\n",
      "48,964.00\n",
      "48,1220.00\n",
      "48,1432.00\n",
      "48,21.00\n",
      "48,1164.00\n",
      "48,1607.00\n",
      "48,1724.00\n",
      "48,31.00\n",
      "48,1078.00\n",
      "48,1395.00\n",
      "48,1602.00\n",
      "48,24.00\n",
      "48,1486.00\n",
      "48,1687.00\n",
      "48,1949.00\n",
      "48,23.00\n",
      "48,1116.00\n",
      "48,1355.00\n",
      "48,1508.00\n",
      "48,26.00\n",
      "48,1170.00\n",
      "48,1531.00\n",
      "48,1860.00\n",
      "48,22.00\n",
      "48,948.00\n",
      "48,1275.00\n",
      "48,1490.00\n",
      "48,29.00\n",
      "48,1090.00\n",
      "48,1587.00\n",
      "48,1627.00\n",
      "48,26.00\n",
      "48,1102.00\n",
      "48,1323.00\n",
      "48,1700.00\n",
      "48,25.00\n",
      "48,848.00\n",
      "48,1058.00\n",
      "48,1272.00\n",
      "48,22.00\n",
      "48,1226.00\n",
      "48,1545.00\n",
      "48,1773.00\n",
      "48,25.00\n",
      "48,934.00\n",
      "48,1298.00\n",
      "48,1419.00\n",
      "48,21.00\n",
      "48,1029.00\n",
      "48,1247.00\n",
      "48,1340.00\n",
      "48,27.00\n",
      "48,772.00\n",
      "48,1079.00\n",
      "48,1228.00\n",
      "48,25.00\n",
      "48,884.00\n",
      "48,1217.00\n",
      "48,1427.00\n",
      "48,22.00\n",
      "48,873.00\n",
      "48,1053.00\n",
      "48,1216.00\n",
      "48,23.00\n",
      "48,844.00\n",
      "48,1119.00\n",
      "48,1229.00\n",
      "48,17.00\n",
      "48,1161.00\n",
      "48,1408.00\n",
      "48,1681.00\n",
      "48,21.00\n",
      "48,1006.00\n",
      "48,1234.00\n",
      "48,1511.00\n",
      "48,17.00\n"
     ]
    }
   ],
   "source": [
    "for line in f:\n",
    "    log_line = json.loads(line.strip(\"\\n\").replace(\"\\n\", \"\\\\n\"))\n",
    "    if log_line[\"Event\"]=='SparkListenerStageCompleted':\n",
    "        number_tasks = log_line[\"Stage Info\"][\"Number of Tasks\"]\n",
    "        submission_time = log_line[\"Stage Info\"][\"Submission Time\"]\n",
    "        completion_time = log_line[\"Stage Info\"][\"Completion Time\"]\n",
    "        print \"%d,%.2f\"%(number_tasks, completion_time-submission_time)\n",
    "    elif log_line[\"Event\"]=='SparkListenerTaskCompleted':\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Event Types:\n",
    "\n",
    "* 'SparkListenerApplicationStart': 1,\n",
    "* 'SparkListenerBlockManagerAdded': 2,\n",
    "* 'SparkListenerEnvironmentUpdate': 1,\n",
    "* 'SparkListenerExecutorAdded': 1,\n",
    "* 'SparkListenerJobStart': 1,\n",
    "* 'SparkListenerLogStart': 1,\n",
    "* 'SparkListenerStageCompleted': 1,\n",
    "* 'SparkListenerStageSubmitted': 1,\n",
    "* 'SparkListenerTaskEnd': 48,\n",
    "* 'SparkListenerTaskStart': 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Submission Time\": 1501974360148, \"Stage IDs\": [0], \"Properties\": {\"callSite.short\": \"call at /home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:2230\", \"spark.rdd.scope\": \"{\\\"id\\\":\\\"2\\\",\\\"name\\\":\\\"collect\\\"}\", \"spark.rdd.scope.noOverride\": \"true\"}, \"Job ID\": 0, \"Stage Infos\": [{\"Stage Attempt ID\": 0, \"Number of Tasks\": 48, \"Parent IDs\": [], \"Stage ID\": 0, \"RDD Info\": [{\"RDD ID\": 2, \"Parent IDs\": [1], \"Name\": \"PythonRDD\", \"Storage Level\": {\"Use Disk\": false, \"Replication\": 1, \"Use Memory\": false, \"Deserialized\": false}, \"Disk Size\": 0, \"Callsite\": \"call at /home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:2230\", \"Memory Size\": 0, \"Number of Partitions\": 48, \"Number of Cached Partitions\": 0}, {\"RDD ID\": 0, \"Parent IDs\": [], \"Name\": \"KafkaRDD\", \"Storage Level\": {\"Use Disk\": false, \"Replication\": 1, \"Use Memory\": false, \"Deserialized\": false}, \"Disk Size\": 0, \"Callsite\": \"createDirectStreamWithoutMessageHandler at NativeMethodAccessorImpl.java:0\", \"Memory Size\": 0, \"Number of Partitions\": 48, \"Scope\": \"{\\\"id\\\":\\\"0_1501974360000\\\",\\\"name\\\":\\\"kafka direct stream [0]\\\\n@ 18:06:00\\\"}\", \"Number of Cached Partitions\": 0}, {\"RDD ID\": 1, \"Parent IDs\": [0], \"Name\": \"PythonRDD\", \"Storage Level\": {\"Use Disk\": false, \"Replication\": 1, \"Use Memory\": false, \"Deserialized\": false}, \"Disk Size\": 0, \"Callsite\": \"RDD at PythonRDD.scala:48\", \"Memory Size\": 0, \"Number of Partitions\": 48, \"Number of Cached Partitions\": 0}], \"Stage Name\": \"call at /home/01131/tg804093/work/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py:2230\", \"Details\": \"org.apache.spark.rdd.RDD.collect(RDD.scala:935)\\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\njava.lang.reflect.Method.invoke(Method.java:498)\\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\npy4j.Gateway.invoke(Gateway.java:280)\\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\npy4j.commands.CallCommand.execute(CallCommand.java:79)\\npy4j.GatewayConnection.run(GatewayConnection.java:214)\\njava.lang.Thread.run(Thread.java:745)\", \"Accumulables\": []}], \"Event\": \"SparkListenerJobStart\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({u'SparkListenerApplicationStart': 1,\n",
       "         u'SparkListenerBlockManagerAdded': 2,\n",
       "         u'SparkListenerEnvironmentUpdate': 1,\n",
       "         u'SparkListenerExecutorAdded': 1,\n",
       "         u'SparkListenerJobStart': 1,\n",
       "         u'SparkListenerLogStart': 1})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "with open(LOG_NAME, \"r\") as f:\n",
    "    events = []\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        log_line = json.loads(line.strip(\"\\n\").replace(\"\\n\", \"\\\\n\"))\n",
    "        events.append(log_line[\"Event\"])\n",
    "        if log_line[\"Event\"]==\"SparkListenerJobStart\":\n",
    "            print json.dumps(log_line)\n",
    "            break\n",
    "        elif log_line[\"Event\"]==\"SparkListenerTaskEnd\":\n",
    "            num = log_line[\"Task Metrics\"][\"Input Metrics\"][\"Records Read\"]\n",
    "            print str(num)\n",
    "            #print json.dumps(log_line)\n",
    "            count += 1\n",
    "            \n",
    "        #if log_line[\"Event\"]=='SparkListenerTaskEnd':\n",
    "        #    print log_line[\"Event\"]\n",
    "        #    #print str(log_line.keys())\n",
    "        #    #submission_time = log_line[\"Stage Info\"][\"Submission Time\"]\n",
    "        #    #completion_time = log_line[\"Stage Info\"][\"Completion Time\"]\n",
    "        #    #print \"%d,%.2f\"%(number_tasks, completion_time-submission_time)\n",
    "        #elif log_line[\"Event\"]=='SparkListenerTaskCompleted':\n",
    "        #    pass\n",
    "        \n",
    "Counter(events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c251-107.wrangler.tacc.utexas.edu\r\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
