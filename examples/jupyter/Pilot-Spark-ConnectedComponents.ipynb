{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDAnalysis and Pilot-In-Memory\n",
    "\n",
    "\n",
    "The main performance bottleneck of the current MDAnalysis implementation is the construction of the graph using NetworkX taking ~78% of the overall runtime.\n",
    "\n",
    "\n",
    "**Beckstein Profiling:**\n",
    "\n",
    "    47        10           33      3.3      0.0      if adj is None:\n",
    "    48        10        66544   6654.4      0.0          x = atoms.positions\n",
    "\n",
    "    54        10     58689221 5868922.1     18.8          adj = (MDAnalysis.core.parallel.distances.distance_array(x, x, box=box) < cutoff)\n",
    "    \n",
    "    58        10           78      7.8      0.0      adjk = adj if Nmax is None else adj[:Nmax, :Nmax] \n",
    "    59        10    243009076 24300907.6   77.9      graph = nx.Graph(adjk)\n",
    "    60        10      4346636 434663.6      1.4      subgraphs = nx.connected_components(graph)\n",
    "    61        49        83597   1706.1      0.0      indices = [np.sort(g) for g in subgraphs]\n",
    "    62        49      5694698 116218.3      1.8      return [atoms[group].residues for group in indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LeafletFinder NetworkX Implementation Profiling\n",
    "\n",
    "see https://code.google.com/p/mdanalysis/\n",
    "\n",
    "Profile default implementation based on [NetworkX](https://networkx.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "FILENAME=\"../data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "!head -n 5 {FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%time\n",
    "import networkx as NX\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "start = time.time()\n",
    "nxg = NX.read_edgelist(FILENAME, delimiter=\",\")\n",
    "end_read = time.time()\n",
    "NX.draw(nxg, pos=NX.spring_layout(nxg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "degree_sequence=sorted(NX.degree(nxg).values(),reverse=True) # degree sequence\n",
    "print \"Degree sequence\", degree_sequence\n",
    "print \"Length: %d\" % len(degree_sequence)\n",
    "\n",
    "dmax=max(degree_sequence)\n",
    "\n",
    "plt.loglog(degree_sequence,'b-',marker='o')\n",
    "plt.title(\"Degree Histogram\")\n",
    "plt.ylabel(\"Degree\")\n",
    "plt.xlabel(\"Node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "components = NX.connected_components(nxg)\n",
    "end_created = time.time()\n",
    "count = 0\n",
    "for component in components:\n",
    "    print str(sorted(component))\n",
    "    count = count + 1\n",
    "end_connected = time.time()\n",
    "print (\"Number of Nodes: \" + str(NX.number_of_nodes(nxg)))\n",
    "print (\"Number of Edges: \" + str(NX.number_of_edges(nxg)))\n",
    "print (\"Connected Components: \" + str(count))\n",
    "print (\"Runtime: \" + str((end_connected-start)))\n",
    "print (\"Graph Creation Runtime: \" + str((end_created-start)))\n",
    "print (\"Connected Components Runtime: \" + str((end_connected - end_created)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pilot_hadoop import PilotComputeService\n",
    "from IPython.display import HTML\n",
    "\n",
    "os.environ[\"SAGA_VERBOSE\"]=\"100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pilot-Spark and Pilot-InMemory Implementation\n",
    "\n",
    "Setup Spark cluster on local machine or HPC resource\n",
    "\n",
    "### 2.1 Start Spark Cluster using Pilot-Spark\n",
    "\n",
    "see https://github.com/drelu/saga-hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pilot_compute_description = {\n",
    "                            \"resource_url\":\"fork://localhost\",\n",
    "                            \"number_cores\": 1,\n",
    "                            \"cores_per_node\":1,\n",
    "                            \"type\":\"spark\"\n",
    "                            }\n",
    "pilot = PilotComputeService.create_pilot(pilot_compute_description);\n",
    "\n",
    "# print out details of Pilot-Spark\n",
    "details = pilot.get_details()\n",
    "HTML(\"<a target='blank' href='%s'>Spark Web UI</a>\"%details[\"web_ui_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Utilize Spark Native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sc = pilot.get_spark_context()\n",
    "sc.version\n",
    "\n",
    "rdd = sc.textFile(\"/path/to/leaflet_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Pilot-InMemory Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "du = DistributedInMemoryDataUnit(name=\"LeafletFinderGraph\", sc=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from distributed_inmem.dataunit_spark import DistributedInMemoryDataUnit\n",
    "import time\n",
    "\n",
    "FILENAME=\"../data/mdanalysis/small/graph_edges_95_215.csv\"\n",
    "\n",
    "FILENAME_ALL_EDGES=\"../data/mdanalysis/small/graph_edges_95_215_alledges.csv\"\n",
    "du = DistributedInMemoryDataUnit(name=\"LeafletFinderGraph\", sc=sc)\n",
    "\n",
    "#DistributedInMemoryDataUnit.spark_context.version\n",
    "\n",
    "f = open(FILENAME_ALL_EDGES)\n",
    "graph = f.readlines()\n",
    "du.load(graph)\n",
    "f.close()\n",
    "\n",
    "def identityMapper(edge, args):\n",
    "    #print edge\n",
    "    #comp = edge.strip().split(\",\")\n",
    "    #return (int(comp[0]), int(comp[1]))\n",
    "    return eval(str(edge))\n",
    "\n",
    "def groupByVertex(data):\n",
    "    print(\"Call reduce on: \" + str(data))\n",
    "    \n",
    "\n",
    "new_iteration_needed = du.sc.accumulator(0)\n",
    "\n",
    "# check for smaller keys in each set\n",
    "def process_vertex(vertex):\n",
    "    \"\"\" pass single vertex and its adjecent vertices\n",
    "        e.g.: (0, [0, 67, 14])\n",
    "    \"\"\"\n",
    "    global new_iteration_needed\n",
    "    vertex = eval(vertex)\n",
    "    source = int(vertex[0])\n",
    "    dest= sorted([int(i) for i in vertex[1]])\n",
    "    local_max = False\n",
    "    \n",
    "    first_edge_destination = int(dest[0])\n",
    "    new_vertices = []    \n",
    "    print \"*********Source: %d First Edge Dest: %d\"%(source, first_edge_destination) \n",
    "    if source <= first_edge_destination:\n",
    "        local_max = True\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "            \n",
    "\n",
    "    print \"Process: \" + str(vertex) + \" Local Max: \" + str(local_max)\n",
    "    last_edge_destination = first_edge_destination\n",
    "\n",
    "    for current_destination in vertex[1]:\n",
    "        print \"Current destination: %s\"%str(current_destination)\n",
    "        current_destination = int(current_destination)\n",
    "        if current_destination == last_edge_destination: \n",
    "            continue\n",
    "        \n",
    "        if local_max == True:\n",
    "            edge = (source, current_destination)\n",
    "            new_vertices.append(edge)\n",
    "        else:\n",
    "            new_vertices.append((first_edge_destination, current_destination))\n",
    "            new_vertices.append((current_destination, first_edge_destination))\n",
    "            print \"Add 1 to accumulator\"\n",
    "            new_iteration_needed.add(1)\n",
    "\n",
    "        last_edge_destination = current_destination\n",
    "    \n",
    "    if ((not local_max) and (source < last_edge_destination)):\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "    \n",
    "    print \"Return new vertices: \" + str(new_vertices)\n",
    "    return new_vertices\n",
    "\n",
    "\n",
    "#process_vertex(\"('19', ['19', '7', '9', '41'])\")\n",
    "num_iterations=0\n",
    "start = time.time()\n",
    "while True:\n",
    "    old_accum_value = new_iteration_needed.value\n",
    "    print \"*********** Start iteration: %d \" % num_iterations\n",
    "    future_result = du.map_pilot(identityMapper, None, number_of_compute_units=2)\n",
    "    result_du=future_result.result()[0]\n",
    "    future_result = result_du.reduce_pilot(process_vertex, number_of_compute_units=2)\n",
    "    output = future_result.result()\n",
    "    output.export()   \n",
    "    du = output\n",
    "    num_iterations = num_iterations + 1\n",
    "    print \"New iteration accum: %d old value: %d\"%(new_iteration_needed.value, old_accum_value)\n",
    "    if old_accum_value < new_iteration_needed.value:\n",
    "        #print \"Accumulator value was increased. New iteration.\"\n",
    "        continue        \n",
    "        #pass\n",
    "    else:\n",
    "        break\n",
    "    break\n",
    "end = time.time()\n",
    "print \"Final results: \"\n",
    "num_components=du.data.groupByKey().count()\n",
    "print \"Finished after %d Iterations. Found %d components. Time: %.2f\"%(num_iterations, num_components, (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "du.data.groupByKey().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Native Spark Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# Mac OS:\n",
    "# brew install apache-spark\n",
    "# SPARK_HOME='/usr/local/Cellar/apache-spark/1.3.0/libexec/'\n",
    "# Start Spark: /usr/local/Cellar/apache-spark/1.3.0/libexec/sbin/start-all.sh\n",
    "# py4j needs be installed in your virtualenv\n",
    "from pyspark import SparkContext, SparkConf, Accumulator, AccumulatorParam\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.linalg import Vector\n",
    "\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    sc = SparkContext(\"local[1]\")\n",
    "    sqlCtx=SQLContext(sc)\n",
    "\n",
    "print \"Loaded Spark: %s\"%(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Load data from text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = sc.textFile(FILENAME).map(lambda line: [int(i) for i in line.split(\",\")])\n",
    "# add backward edges\n",
    "data = data.flatMap(lambda v: [(v[0],v[1]),(v[1],v[0])])\n",
    "\n",
    "#data.saveAsTextFile(\"../data/mdanalysis/small/graph_edges_95_215_alledges.csv\")\n",
    "#data = data.filter(lambda v: v[0] != v[1])\n",
    "#print data.collect()\n",
    "\n",
    "data_grouped = data.groupByKey().mapValues(lambda a: sorted(set(a)))\n",
    "print data_grouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Connected Component Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "new_iteration_needed = sc.accumulator(0)\n",
    "# check for smaller keys in each set\n",
    "def process_vertex(vertex):\n",
    "    \"\"\" pass single vertex and its adjecent vertices\n",
    "        e.g.: (0, [0, 67, 14])\n",
    "    \"\"\"\n",
    "    global new_iteration_needed\n",
    "    source = vertex[0]\n",
    "    local_max = False\n",
    "    \n",
    "    first_edge_destination = vertex[1][0]\n",
    "    new_vertices = []    \n",
    "    print \"*********Source: %d First Edge Dest: %d\"%(source, first_edge_destination) \n",
    "    if source <= first_edge_destination:\n",
    "        local_max = True\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "            \n",
    "    #pdb.set_trace()\n",
    "    print \"Process: \" + str(vertex) + \" Local Max: \" + str(local_max)\n",
    "    last_edge_destination = first_edge_destination\n",
    "\n",
    "    #if vertex[1]==None or len(vertex[1])<=1:\n",
    "    #    new_vertices.append((source, source))   \n",
    "    for current_destination in vertex[1]:\n",
    "        #print \"Current destination: %s\"%str(current_destination)\n",
    "        if current_destination == last_edge_destination: \n",
    "            continue\n",
    "        \n",
    "        if local_max == True:\n",
    "            edge = (source, current_destination)\n",
    "            new_vertices.append(edge)\n",
    "        else:\n",
    "            new_vertices.append((first_edge_destination, current_destination))\n",
    "            new_vertices.append((current_destination, first_edge_destination))\n",
    "            print \"Add 1 to accumulator\"\n",
    "            new_iteration_needed.add(1)\n",
    "\n",
    "        last_edge_destination = current_destination\n",
    "    \n",
    "    if ((not local_max) and (source < last_edge_destination)):\n",
    "        new_vertices.append((source, first_edge_destination))\n",
    "    \n",
    "    #print \"Return new vertices: \" + str(new_vertices)\n",
    "    return new_vertices\n",
    "\n",
    "\n",
    "#process_vertex((19, [7, 9, 19, 41]))\n",
    "num_iterations=0\n",
    "cc = data_grouped\n",
    "start = time.time()\n",
    "while True:\n",
    "    old_accum_value = new_iteration_needed.value\n",
    "    print \"*********** Start iteration: %d \" % num_iterations\n",
    "    #print \"Accum before iteration: \" + str(old_accum_value)\n",
    "    cc = cc.flatMap(lambda v: process_vertex(v))\\\n",
    "           .groupByKey()\\\n",
    "           .mapValues(lambda a: sorted(set(a)))\n",
    "    cc.collect()\n",
    "    num_iterations = num_iterations + 1\n",
    "    #print \"New iteration accum: %d old value: %d\"%(new_iteration_needed.value, old_accum_value)\n",
    "    if old_accum_value < new_iteration_needed.value:\n",
    "        #print \"Accumulator value was increased. New iteration.\"\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "end = time.time()\n",
    "\n",
    "print \"Finished after %d Iterations. Found %d components. Time: %.2f\"%(num_iterations, cc.count(), (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Scratch Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#start = df_grouped\n",
    "result=start.flatMap(lambda v: (v[0], v[1])).map(lambda v: v<start_index).countByValue()\n",
    "\n",
    "print sttrresult\n",
    "\n",
    "local_max = not result.has_key(True)\n",
    "\n",
    "print \"Local Max: \" + str(local_max) + \" Smaller Index: \" + str(result.has_key(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "            StructField(\"source\", IntegerType(), True),\n",
    "            StructField(\"destination\", IntegerType(), True)\n",
    "        ])\n",
    "df = sqlCtx.createDataFrame(data, schema)\n",
    "df.explain()\n",
    "schema_grouped = StructType([\n",
    "            StructField(\"source\", IntegerType(), True),\n",
    "            StructField(\"destination\", ArrayType(IntegerType()), True)\n",
    "        ])\n",
    "df_grouped = sqlCtx.createDataFrame(data_grouped, schema_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "t = udf(lambda s: str(s), StringType())\n",
    "slen = udf(lambda s: Column(len(s)), IntegerType())\n",
    "\n",
    "#df.groupBy(\"source\").collect()\n",
    "#df.groupBy(\"source\").agg(df.source, t(df.source))\n",
    "\n",
    "c = df.groupBy(df.source).agg(col(\"source\"), slen(df.destination))\n",
    "\n",
    "#c = df.agg(col(\"source\"), t(df.destination).alias('counts'))\n",
    "c.head(5)\n",
    "\n",
    "\n",
    "#c = df.groupBy(df.source).lit(df.destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "vertices = df.select(df[\"source\"]).unionAll(df.select(df[\"destination\"]))\n",
    "vertices = di_source.distinct()\n",
    "\n",
    "print \"Number of vertices: %d\"%(vertices.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from graphlab import SGraph, SFrame\n",
    "from graphlab import connected_components\n",
    "\n",
    "PROBLEM={\"small\": \"./data/mdanalysis/small/graph_edges_95_215.csv\",\n",
    "         \"medium\":\"./data/mdanalysis/medium/graph_edges_24056_71826.csv\"}\n",
    "\n",
    "d =datetime.datetime.now()\n",
    "RESULTSFILE = \"results-\" + d.strftime(\"%Y%m%d-%H%M%S\") + \".csv\"\n",
    "REPEATS=5\n",
    "\n",
    "start = time.time()\n",
    "data = SFrame.read_csv(filename, header=False)\n",
    "sg = SGraph().add_edges(data, src_field=\"X1\", dst_field=\"X2\")\n",
    "end_read=time.time()\n",
    "cc = connected_components.create(sg)\n",
    "s=cc[\"component_size\"]\n",
    "end_connected = time.time()\n",
    "print cc\n",
    "print s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6.0,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}